{"name":"AMIDST Toolbox 2.0","tagline":"A Java Toolbox for Analysis of MassIve Data STreams using Probabilistic Graphical Models","body":"# Toolbox Functionalities<a name=\"functionalities\"></a>\r\n\r\nThe AMIDST system is an open source Java 8 toolbox that makes use of a functional programming \r\nstyle to support parallel processing on mutli-core CPUs (Masegosa et al., 2015). \r\nAMIDST provides a collection of functionalities and algorithms for learning both static and dynamic \r\nhybrid Bayesian networks from streaming data. \r\n\r\nIn what follows, we describe the main functionalities that the AMIDST toolbox supplies.\r\n\r\n##AMIDST Core Structures<a name=\"pgms\"></a>\r\n\r\nAn overview of the core structures of the AMIDST toolbox is illustrated in the figure below. \r\nBlue boxes represent software components that have been fully implemented, while green boxes represent components that are part of AMIDST design specification to be implemented in the future.\r\n\r\n<p align=\"center\">\r\n<img title=\"Illustration of the design of the AMIDST components related to core structures. Nomenclature: The boxes in the figure represent software components (sets, possibly singletons, of classes), a rounded-arc going from X to Y indicate that Y ‘uses/references’ X, and an arc with an arrow from X to Y implies inheritance.\r\n\" src=\"https://amidst.github.io/toolbox/docs/PGMs.pdf\" width=\"700\">\r\n</p>\r\n\r\nThe core structures mainly relates to the basic components that play a \r\nkey role in ensuring the implementation of the different AMIDST learning and inference algorithms.\r\n\r\nFor starters, instantiation of a particular **probabilistic graphical model** (**PGM** component) will be required. \r\nCurrently, it is possible to create either a static Bayesian network (**BN** component) or a two time-slice dynamic \r\nBayesian network (**2T-DBN** component). Then, the **DAG** component is defined over a list of **Static Variables**, whereas \r\nthe **2T-DBN** component is defined over a list of **Dynamic Variables**.\r\n\r\nBoth **BN** and **2T-DBN** models rely on the **Distributions** component to define the conditional probability distributions. \r\nThis component supports both **Conditional Linear Gaussians** and **Exponential Family** representations of the distributions. \r\nThe implementation of the latter ensures an alternative representation of the standard distributions based on vectors of natural and \r\nmoment parameters.\r\n\r\n##AMIDST modules<a name=\"modules\"></a> \r\n\r\nThe following figure shows a high-level overview of the key modules of the AMIDST toolbox:\r\n\r\n<p align=\"center\">\r\n<img title=\"Illustration of the design of the AMIDST components related to core structures.\" src=\"https://amidst.github.io/toolbox/docs/Functionalities.pdf\" width=\"700\">\r\n</p>\r\n\r\n###Data Sources<a name=\"datasources\"></a> \r\n\r\nIn the AMIDST framework, we consider two types of data sources for learning, namely, **DataStream**, \r\nwhere data arrives at high frequency with no storage of historical data, and **DataOnMemory** for static \r\ndatabases that simply correspond to traditional databases. \r\nThe data format supported by AMIDST is [Weka](www.cs.waikato.ac.nz/ml/weka/)’s attribute-relation file format (ARFF).\r\n\r\nMoreover, we ensure a scalable **DistributedDataProcessing** using [Apache Flink](http://flink.apache.org) that runs the AMIDST developed algorithms \r\non top of the Hadoop and Yarn architectures on [Amazon Elastic MapReduce (EMR)](https://aws.amazon.com/elasticmapreduce/). This part will be soon released!\r\n\r\nEach of the data sources are furthermore connected to the **DataInstance** and **DynamicDataInstance** components \r\nthat represent a particular evidence configuration (static or dynamic, respectively) such as the observed values \r\nof a collection of variables at time t or a particular row in a database.\r\n\r\n###Inference Engines<a name=\"inference\"></a>\r\n\r\n* **The Static Inference Engine** includes all the inference algorithms for a static Bayesian networks (BNs). \r\nIt consists of five implemented methods, namely, **HUGIN Exact Inference** (Madsen et al., 2005), \r\n**Importance Sampling** (Hammersley and Handscomb, 1964; Salmeron et al., 2015), \r\n**Variational Message Passing (VMP)** (Winn and Bishop, 2005), **Static MAP Inference**, and **MPE Inference**. \r\nThis module has been designed and implemented to be extendable and support future implementations of other \r\ninference algorithms such as the **Expectation Propagation (EP)** algorithm. \r\n\r\n* **The Dynamic Inference Engine** includes all the inference algorithms for dynamic Bayesian networks (DBNs). \r\nIt consists of the implementation of the dynamic versions of HUGIN Exact Inference, Importance Sampling, \r\nVariational Message Passing by means of the **Factored Frontier** algorithm, as well as **Dynamic MAP**. \r\nThe latter is implemented by computing (variational) posterior distributions over subsets of MAP variables. \r\n\r\n\r\n###Learning Engine<a name=\"learning\"></a>  \r\n\r\nLearning engine includes the **Structural Learning**, **Parameter Learning**, and **Feature Selection** (still under development) \r\ncomponents that are connected to both PGM and DataStream. \r\n\r\n* **The Structural Learning** component currently includes parallel implementations of the tree augmented naive Bayes \r\nclassifier (**Parallel TAN**) (Madsen et al., 2014) as well as a constraint-based method for learning general \r\nBayesian networks (**Parallel PC**) (Madsen et al., 2015). Both of the these implementations are based on the use of threads, \r\nand rely on interfacing to the Hugin AMIDST API.\r\n\r\n* **The Parameter Learning** in AMIDST models can be performed using either a **fully Bayesian approach** \r\nor a **maximum likelihood-based approach** (Scholz, 2004). The Bayesian approach to parameter learning is closely linked to the \r\ntask of probabilistic inference supported by the Streaming Variational Bayes algorithm (Broderick et al., 2013), using Variational Message Passing as the underlying inference \r\nalgorithm (VMP) provided by the Static Inference Engine in the toolbox.\r\n\r\n##Links to MOA and Hugin<a name=\"librarylinks\"></a> \r\nAMIDST leverages existing functionalities and algorithms by interfacing to software tools such as \r\n[Hugin](http://www.hugin.com), and [MOA](http://moa.cms.waikato.ac.nz) (Massive Online Analysis). \r\nThis allows the toolbox to efficiently exploit well-established systems and also broaden the AMIDST \r\nuser-base. \r\n\r\n* **HuginLink** consists of a set of functionalities implemented to link the AMIDST toolbox \r\nwith [Hugin](http://www.hugin.com) commercial software. \r\nThis connection extends AMIDST by providing some of the main functionalities \r\nof [Hugin](http://www.hugin.com), such as exact inference algorithms and scalable structural learning \r\nalgorithms (Madsen et al., 2014). [Hugin](http://www.hugin.com) is a third-party commercial software \r\nand to access to these functionalities it is needed a license of the software and to follow some specific \r\ninstallation steps (further information is given [here](http://amidst.github.io/toolbox/#installhugin)).\r\n\r\n* **MoaLink** ensures an easy use of AMDIST functionalities within [MOA](http://moa.cms.waikato.ac.nz). \r\nThe main idea is that any model deployed in AMIDST can be integrated and evaluated using MOA's graphical user interface. \r\nAs a proof of concept, MoaLink already provides a classification, a regression and a clustering method based on \r\nBN models with latent variables. These models are learnt in a streaming fashion using AMIDST Learning Engine. \r\n\r\n##Concept drift<a name=\"conceptdrift\"></a> \r\nAMIDST also offers some support for dealing with concept drift while learning BNs from data streams. \r\nMore precisely, the toolbox supports a novel probabilistic approach based on latent variables \r\n(Borchani et al., 2015) for detecting and adapting to concept drifts.\r\n\r\n###Bibliography\r\n\r\nHanen Borchani, Ana M. Martinez, Andres R. Masegosa, et al. \r\nModeling concept drift: A probabilistic graphical model based approach. \r\nIn The Fourteenth International Symposium on Intelligent Data Analysis, pages 72-83, 2015.\r\n\r\nTamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. \r\nStreaming variational Bayes. \r\nIn Advances in Neural Information Processing Systems, pages 1727–1735, 2013.\r\n\r\nJ.M. Hammersley and D.C. Handscomb. \r\nMonte Carlo Methods. \r\nMethuen & Co, London, UK, 1964.\r\n\r\nAnders L. Madsen, Frank Jensen, Uffe B. Kjærulff, and Michael Lang. \r\nHUGIN-the tool for Bayesian networks and influence diagrams. \r\nInternational Journal of Artificial Intelligence Tools, 14(3):507–543, 2005.\r\n\r\nAnders L. Madsen, Frank Jensen, Antonio Salmeron, et al. \r\nA new method for vertical parallelisation of TAN learning Based on balanced incomplete block designs. \r\nIn The Seventh European Workshop on Probabilistic Graphical Models, pages 302–317, 2014.\r\n\r\nAnders L. Madsen, Frank Jensen, Antonio Salmeron, et al. \r\nParallelization of the PC Algorithm. \r\nIn The XVI Conference of the Spanish Association for Artificial Intelligence, pages 14-24, 2015.\r\n\r\nAndres R. Masegosa, Ana M. Martinez, and Hanen Borchani. \r\nProbabilistic graphical models on multi-core CPUs using Java 8. \r\nIEEE Computational Intelligence Magazine, Special Issue on Computational Intelligence Software, Under review, 2015.\r\n\r\nAntonio Salmeron, Dario Ramos-Lopez, Hanen Borchani, et al. \r\nParallel importance sampling in conditional linear Gaussian networks. \r\nIn The XVI Conference of the Spanish Association for Artificial Intelligence, pages 36-46, 2015.\r\n\r\nF. W. Scholz. Maximum Likelihood Estimation. John Wiley & Sons, Inc., 2004.\r\n\r\nJohn M. Winn and Christopher M. Bishop. \r\nVariational message passing. Journal of Machine Learning Research, 6:661–694, 2005.","google":"UA-66233470-1","note":"Don't delete this file! It's used internally to help with page regeneration."}