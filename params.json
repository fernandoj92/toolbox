{"name":"AMIDST Toolbox 1.0","tagline":"A Java Toolbox for Analysis of MassIve Data STreams using Probabilistic Graphical Models","body":"## Dynamic Bayesian networks: code Examples<a name=\"documentation\"></a>\r\n\r\n   * [Dynamic Data Streams](#dynamicdatastreamsexample)\r\n   * [Dynamic Random Variables](#dynamicvariablesexample)\r\n   * [Dynamic Bayesian Networks](#dynamicbnexample)\r\n       * [Creating Bayesian Networks](#dynamicbnnohiddenexample)\r\n       * [Creating Bayesian Networks with latent variables](#dynamicbnhiddenexample)\r\n       * [Modifying Bayesian Networks](#dynamicbnmodifyexample)\r\n   * [Dynamic I/O Functionality](#dynamicioexample)\r\n       * [I/O of Data Streams](#dynamiciodatastreamsexample)\r\n       * [I/O of Bayesian Networks](#dynamiciobnsexample)\r\n   * [Dynamic Inference Algorithms](#dynamicinferenceexample)\r\n       * [The Inference Engine](#dynamicinferenceengingeexample)\r\n       * [Variational Message Passing](#dynamicvmpexample)\r\n       * [Importance Sampling](#dynamicisexample)\r\n   * [Dynamic Learning Algorithms](#dynamiclearningexample)\r\n       * [Maximum Likelihood](#dynamicmlexample)\r\n       * [Parallel Maximum Likelihood](#dynamicpmlexample)\r\n       * [Streaming Variational Bayes](#dynamicsvbexample)\r\n       * [Parallel Streaming Variational Bayes](#dynamicpsvbexample)\r\n\r\n\r\n## Dynamic Data Streams<a name=\"dynamicdatastreamsexample\"></a>\r\n  \r\nIn this example we show how to use the main features of a *DataStream* object. More precisely,  we show  how to load a dynamic data stream and how to iterate over the *DynamicDataInstance* objects.\r\n\r\n```java\r\n//Open the data stream using the class DynamicDataStreamLoader\r\nDataStream<DynamicDataInstance> data = DynamicDataStreamLoader.loadFromFile(\"datasets/dynamicNB-samples.arff\");\r\n\r\n//Access the attributes defining the data stream\r\nSystem.out.println(\"Attributes defining the data set\");\r\nfor (Attribute attribute : data.getAttributes()) {\r\n    System.out.println(attribute.getName());\r\n}\r\nAttribute classVar = data.getAttributes().getAttributeByName(\"ClassVar\");\r\n\r\n//Iterate over dynamic data instances\r\nSystem.out.println(\"1. Iterating over samples using a for loop\");\r\nfor (DynamicDataInstance dataInstance : data) {\r\n    System.out.println(\"SequenceID = \"+dataInstance.getSequenceID()+\", TimeID = \"+dataInstance.getTimeID());\r\n    System.out.println(\"The value of attribute A for the current data instance is: \" +\r\n    \tdataInstance.getValue(classVar));\r\n}\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## Dynamic Random Variables<a name=\"dynamicvariablesexample\"></a>\r\n\r\nThis example show the basic functionalities related to dynamic variables.\r\n\r\n```java\r\n//Create an empty DynamicVariables object\r\nDynamicVariables variables = new DynamicVariables();\r\n\r\n//Invoke the \"new\" methods of the object DynamicVariables to create new dynamic variables.\r\n\r\n//Create a Gaussian dynamic variables\r\nVariable gaussianVar = variables.newGaussianDynamicVariable(\"GaussianVar\");\r\n\r\n//Create a Multinomial dynamic variable with two states\r\nVariable multinomialVar = variables.newMultinomialDynamicVariable(\"MultinomialVar\", 2);\r\n\r\n//Create a Multinomial dynamic variable with two states: TRUE and FALSE\r\nVariable multinomialVar2 = variables.newMultinomialDynamicVariable(\"MultinomialVar2\", Arrays.asList(\"TRUE, FALSE\"));\r\n\r\n//All dynamic Variables have an interface variable\r\nVariable gaussianVarInt = gaussianVar.getInterfaceVariable();\r\nVariable multinomialVarInt = multinomialVar.getInterfaceVariable();\r\n\r\n//Get the \"main\" Variable associated with each interface variable through the DynamicVariable object\r\nVariable mainMultinomialVar = variables.getVariableFromInterface(multinomialVarInt);\r\n\r\n//Check whether a variable is an interface variable\r\nSystem.out.println(\"Is Variable \"+gaussianVar.getName()+\" an interface variable? \"\r\n                +gaussianVar.isInterfaceVariable());\r\nSystem.out.println(\"Is Variable \"+gaussianVarInt.getName()+\" an interface variable? \"\r\n                +gaussianVarInt.isInterfaceVariable());\r\n\r\n//Check whether a variable is a dynamic variable\r\nSystem.out.println(\"Is Variable \"+multinomialVar.getName()+\" a dynamic variable? \"\r\n                +gaussianVar.isDynamicVariable());\r\n```\r\n\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n\r\n## Dynamic Bayesian Networks<a name=\"dynamicbnexample\"></a>\r\n\r\n### Creating dynamic Bayesian networks<a name=\"bnnohiddenexample\"></a>\r\n\r\nThis example creates a dynamic BN, from a dynamic data stream, with randomly generated probability distributions, then saves it to a file.\r\n\r\n```java\r\n//Open the data stream using the static class DynamicDataStreamLoader\r\nDataStream<DynamicDataInstance> data = DynamicDataStreamLoader.loadFromFile(\r\n                \"datasets/syntheticDataDiscrete.arff\");\r\n\r\n/**\r\n* 1. Once the data is loaded, we create a random variable for each of the attributes (i.e. data columns)\r\n* in our data.\r\n*\r\n* 2. {@link DynamicVariables} is the class for doing that. It takes a list of Attributes and internally creates\r\n* all the variables. We create the variables using DynamicVariables class to guarantee that each variable\r\n* has a different ID number and make it transparent for the user. Each random variable has an associated\r\n* interface variable.\r\n*\r\n* 3. We can extract the Variable objects by using the method getVariableByName();\r\n*/\r\nDynamicVariables dynamicVariables = new DynamicVariables(data.getAttributes());\r\nDynamicDAG dynamicDAG = new DynamicDAG(dynamicVariables);\r\n\r\nVariable A = dynamicVariables.getVariableByName(\"A\");\r\nVariable B = dynamicVariables.getVariableByName(\"B\");\r\nVariable C = dynamicVariables.getVariableByName(\"C\");\r\nVariable D = dynamicVariables.getVariableByName(\"D\");\r\nVariable E = dynamicVariables.getVariableByName(\"E\");\r\nVariable G = dynamicVariables.getVariableByName(\"G\");\r\n\r\nVariable A_Interface = dynamicVariables.getInterfaceVariable(A);\r\nVariable B_Interface = dynamicVariables.getInterfaceVariable(B);\r\n\r\n//Note that C_Interface and D_Interface are also created although they will not be used\r\n//(we will not add temporal dependencies)\r\n\r\nVariable E_Interface = dynamicVariables.getInterfaceVariable(E);\r\nVariable G_Interface = dynamicVariables.getInterfaceVariable(G);\r\n\r\n// Example of the dynamic DAG structure\r\n// Time 0: Parents at time 0 are automatically created when adding parents at time T\r\ndynamicDAG.getParentSetTimeT(B).addParent(A);\r\ndynamicDAG.getParentSetTimeT(C).addParent(A);\r\ndynamicDAG.getParentSetTimeT(D).addParent(A);\r\ndynamicDAG.getParentSetTimeT(E).addParent(A);\r\ndynamicDAG.getParentSetTimeT(G).addParent(A);\r\ndynamicDAG.getParentSetTimeT(A).addParent(A_Interface);\r\ndynamicDAG.getParentSetTimeT(B).addParent(B_Interface);\r\ndynamicDAG.getParentSetTimeT(E).addParent(E_Interface);\r\ndynamicDAG.getParentSetTimeT(G).addParent(G_Interface);\r\n\r\nSystem.out.println(dynamicDAG.toString());\r\n\r\n/**\r\n* 1. We now create the Dynamic Bayesian network from the previous Dynamic DAG.\r\n*\r\n* 2. The DBN object is created from the DynamicDAG. It automatically looks at the distribution type\r\n* of each variable and their parents to initialize the Distributions objects that are stored\r\n* inside (i.e. Multinomial, Normal, CLG, etc). The parameters defining these distributions are\r\n* properly initialized.\r\n*\r\n* 3. The network is printed and we can have a look at the kind of distributions stored in the DBN object.\r\n*/\r\nDynamicBayesianNetwork dbn = new DynamicBayesianNetwork(dynamicDAG);\r\nSystem.out.printf(dbn.toString());\r\n\r\n/**\r\n* Finally teh Bayesian network is saved to a file.\r\n*/\r\nDynamicBayesianNetworkWriter.saveToFile(dbn, \"networks/DBNExample.bn\");\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n\r\n### Creating Dynamic Bayesian Networks with Latent Variables <a name=\"dynamicbnhiddenexample\"></a>\r\n\r\nIn this example, we simply show how to create a BN model with hidden variables. We simply create a BN for clustering, i.e.,  a naive-Bayes like structure with a single common hidden variable acting as parant of all the observable variables.\r\n \r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = DataStreamLoader.openFromFile(\"datasets/syntheticData.arff\");\r\n\r\n/**\r\n * 1. Once the data is loaded, we create a random variable for each of the attributes (i.e. data columns)\r\n * in our data.\r\n *\r\n * 2. {@link Variables} is the class for doing that. It takes a list of Attributes and internally creates\r\n * all the variables. We create the variables using Variables class to guarantee that each variable\r\n * has a different ID number and make it transparent for the user.\r\n *\r\n * 3. We can extract the Variable objects by using the method getVariableByName();\r\n */\r\nVariables variables = new Variables(data.getAttributes());\r\n\r\nVariable a = variables.getVariableByName(\"A\");\r\nVariable b = variables.getVariableByName(\"B\");\r\nVariable c = variables.getVariableByName(\"C\");\r\nVariable d = variables.getVariableByName(\"D\");\r\nVariable e = variables.getVariableByName(\"E\");\r\nVariable g = variables.getVariableByName(\"G\");\r\nVariable h = variables.getVariableByName(\"H\");\r\nVariable i = variables.getVariableByName(\"I\");\r\n\r\n/**\r\n * 1. We create the hidden variable. For doing that we make use of the method \"newMultionomialVariable\". When\r\n * a variable is created from an Attribute object, it contains all the information we need (e.g.\r\n * the name, the type, etc). But hidden variables does not have an associated attribute\r\n * and, for this reason, we use now this to provide this information.\r\n *\r\n * 2. Using the \"newMultionomialVariable\" method, we define a variable called HiddenVar, which is\r\n * not associated to any attribute and, then, it is a latent variable, its state space is a finite set with two elements, and its\r\n * distribution type is multinomial.\r\n *\r\n * 3. We finally create the hidden variable using the method \"newVariable\".\r\n */\r\n\r\nVariable hidden = variables.newMultionomialVariable(\"HiddenVar\", Arrays.asList(\"TRUE\", \"FALSE\"));\r\n\r\n/**\r\n * 1. Once we have defined your {@link Variables} object, including the latent variable,\r\n * the next step is to create a DAG structure over this set of variables.\r\n *\r\n * 2. To add parents to each variable, we first recover the ParentSet object by the method\r\n * getParentSet(Variable var) and then call the method addParent(Variable var).\r\n *\r\n * 3. We just put the hidden variable as parent of all the other variables. Following a naive-Bayes\r\n * like structure.\r\n */\r\nDAG dag = new DAG(variables);\r\n\r\ndag.getParentSet(a).addParent(hidden);\r\ndag.getParentSet(b).addParent(hidden);\r\ndag.getParentSet(c).addParent(hidden);\r\ndag.getParentSet(d).addParent(hidden);\r\ndag.getParentSet(e).addParent(hidden);\r\ndag.getParentSet(g).addParent(hidden);\r\ndag.getParentSet(h).addParent(hidden);\r\ndag.getParentSet(i).addParent(hidden);\r\n\r\n/**\r\n * We print the graph to see if is properly created.\r\n */\r\nSystem.out.println(dag.toString());\r\n\r\n/**\r\n * 1. We now create the Bayesian network from the previous DAG.\r\n *\r\n * 2. The BN object is created from the DAG. It automatically looks at the distribution type\r\n * of each variable and their parents to initialize the Distributions objects that are stored\r\n * inside (i.e. Multinomial, Normal, CLG, etc). The parameters defining these distributions are\r\n * properly initialized.\r\n *\r\n * 3. The network is printed and we can have look at the kind of distributions stored in the BN object.\r\n */\r\nBayesianNetwork bn = new BayesianNetwork(dag);\r\nSystem.out.println(bn.toString());\r\n\r\n/**\r\n * Finally teh Bayesian network is saved to a file.\r\n */\r\nBayesianNetworkWriter.saveToFile(bn, \"networks/BNHiddenExample.bn\");\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n\r\n### Modifying Dynamic Bayesian Networks <a name=\"dynamicbnmodifyexample\"></a>\r\n\r\nIn this example we show how to access and modify the conditional probabilities of a Bayesian network model.\r\n\r\n```java\r\n//We first generate a Bayesian network with one multinomial, one Gaussian variable and one link.\r\nBayesianNetworkGenerator.setNumberOfGaussianVars(1);\r\nBayesianNetworkGenerator.setNumberOfMultinomialVars(1,2);\r\nBayesianNetworkGenerator.setNumberOfLinks(1);\r\n\r\nBayesianNetwork bn = BayesianNetworkGenerator.generateBayesianNetwork();\r\n\r\n//We print the randomly generated Bayesian networks\r\nSystem.out.println(bn.toString());\r\n\r\n//We first access the variable we are interested in\r\nVariable multiVar = bn.getStaticVariables().getVariableByName(\"DiscreteVar0\");\r\n\r\n//Using the above variable we can get the associated distribution and modify it\r\nMultinomial multinomial = bn.getConditionalDistribution(multiVar);\r\nmultinomial.setProbabilities(new double[]{0.2, 0.8});\r\n\r\n//Same than before but accessing the another variable\r\nVariable normalVar = bn.getStaticVariables().getVariableByName(\"GaussianVar0\");\r\n\r\n//In this case, the conditional distribtuion is of the type \"Normal given Multinomial Parents\"\r\nNormal_MultinomialParents normalMultiDist = bn.getConditionalDistribution(normalVar);\r\nnormalMultiDist.getNormal(0).setMean(1.0);\r\nnormalMultiDist.getNormal(0).setVariance(1.0);\r\n\r\nnormalMultiDist.getNormal(1).setMean(0.0);\r\nnormalMultiDist.getNormal(1).setVariance(1.0);\r\n\r\n//We print modified Bayesian network\r\nSystem.out.println(bn.toString());\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## I/O Functionality <a name=\"dynamicioexample\"></a>\r\n\r\n### I/O of Data Streams <a name=\"dynamiciodatastreamsexample\"></a>\r\n\r\nIn this example we show how to load and save data sets from [.arff](http://www.cs.waikato.ac.nz/ml/weka/arff.html) files. \r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = DataStreamLoader.openFromFile(\"datasets/syntheticData.arff\");\r\n\r\n//We can save this data set to a new file using the static class DataStreamWriter\r\nDataStreamWriter.writeDataToFile(data, \"datasets/tmp.arff\");\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Dynamic I/O of Bayesian Networks <a name=\"dynamiciobnsexample\"></a>\r\n\r\n\r\nIn this example we show how to load and save Bayesian networks models for a binary file with \".bn\" extension. In this toolbox Bayesian networks models are saved as serialized objects.\r\n\r\n```java\r\n//We can load a Bayesian network using the static class BayesianNetworkLoader\r\nBayesianNetwork bn = BayesianNetworkLoader.loadFromFile(\"./networks/WasteIncinerator.bn\");\r\n\r\n//Now we print the loaded model\r\nSystem.out.println(bn.toString());\r\n\r\n//Now we change the parameters of the model\r\nbn.randomInitialization(new Random(0));\r\n\r\n//We can save this Bayesian network to using the static class BayesianNetworkWriter\r\nBayesianNetworkWriter.saveToFile(bn, \"networks/tmp.bn\");\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## Inference Algorithms <a name=\"dynamicinferenceexample\"></a>\r\n\r\n### The Inference Engine <a name=\"dynamicinferenceengingeexample\"></a>\r\n\r\nThis example show how to perform inference in a Bayesian network model using the InferenceEngine static class. This class aims to be a straigthfoward way to perform queries over a Bayesian network model. By the default the \\textit{VMP} inference method is invoked.\r\n\r\n```java\r\n//We first load the WasteIncinerator bayesian network which has multinomial \r\n//and Gaussian variables.\r\nBayesianNetwork bn = BayesianNetworkLoader.loadFromFile(\"./networks/WasteIncinerator.bn\");\r\n\r\n//We recover the relevant variables for this example: Mout which is normally \r\n//distributed, and W which is multinomial.\r\nVariable varMout = bn.getStaticVariables().getVariableByName(\"Mout\");\r\nVariable varW = bn.getStaticVariables().getVariableByName(\"W\");\r\n\r\n//Set the evidence.\r\nAssignment assignment = new HashMapAssignment(1);\r\nassignment.setValue(varW,0);\r\n\r\n//Then we query the posterior of\r\nSystem.out.println(\"P(Mout|W=0) = \" + InferenceEngine.getPosterior(varMout, bn, assignment));\r\n\r\n//Or some more refined queries\r\nSystem.out.println(\"P(0.7<Mout<6.59 | W=0) = \" + \r\n InferenceEngine.getExpectedValue(varMout, bn, v -> (0.7 < v && v < 6.59) ? 1.0 : 0.0 ));\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Variational Message Passing <a name=\"dynamicvmpexample\"></a>\r\n\r\nThis example we show how to perform inference on a general Bayesian network using the Variational Message Passing (VMP)\r\nalgorithm detailed in\r\n\r\n> Winn, J. M., Bishop, C. M. (2005). Variational message passing. In Journal of Machine Learning Research (pp. 661-694).\r\n\r\n\r\n\r\n```java\r\n//We first load the WasteIncinerator bayesian network which has multinomial \r\n//and Gaussian variables.\r\nBayesianNetwork bn = BayesianNetworkLoader.loadFromFile(\"./networks/WasteIncinerator.bn\");\r\n\r\n//We recover the relevant variables for this example: Mout which is normally \r\n//distributed, and W which is multinomial.\r\nVariable varMout = bn.getStaticVariables().getVariableByName(\"Mout\");\r\nVariable varW = bn.getStaticVariables().getVariableByName(\"W\");\r\n\r\n//First we create an instance of a inference algorithm. In this case, we use \r\n//the VMP class.\r\nInferenceAlgorithm inferenceAlgorithm = new VMP();\r\n\r\n//Then, we set the BN model\r\ninferenceAlgorithm.setModel(bn);\r\n\r\n//If exists, we also set the evidence.\r\nAssignment assignment = new HashMapAssignment(1);\r\nassignment.setValue(varW,0);\r\ninferenceAlgorithm.setEvidence(assignment);\r\n\r\n//Then we run inference\r\ninferenceAlgorithm.runInference();\r\n\r\n//Then we query the posterior of\r\nSystem.out.println(\"P(Mout|W=0) = \" + inferenceAlgorithm.getPosterior(varMout));\r\n\r\n//Or some more refined queries\r\nSystem.out.println(\"P(0.7<Mout<6.59 | W=0) = \" + \r\n inferenceAlgorithm.getExpectedValue(varMout, v -> (0.7 < v && v < 6.59) ? 1.0 : 0.0 ));\r\n\r\n//We can also compute the probability of the evidence\r\nSystem.out.println(\"P(W=0) = \" + Math.exp(inferenceAlgorithm.getLogProbabilityOfEvidence()));\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Importance Sampling <a name=\"isexample\"></a>\r\n\r\nThis example we show how to perform inference on a general Bayesian network using an importance sampling\r\nalgorithm detailed in\r\n\r\n>Fung, R., Chang, K. C. (2013). Weighing and integrating evidence for stochastic simulation in Bayesian networks. arXiv preprint arXiv:1304.1504.\r\n\r\n```java\r\n//We first load the WasteIncinerator bayesian network which has multinomial \r\n//and Gaussian variables.\r\nBayesianNetwork bn = BayesianNetworkLoader.loadFromFile(\"./networks/WasteIncinerator.bn\");\r\n\r\n//We recover the relevant variables for this example: Mout which is normally \r\n//distributed, and W which is multinomial.\r\nVariable varMout = bn.getStaticVariables().getVariableByName(\"Mout\");\r\nVariable varW = bn.getStaticVariables().getVariableByName(\"W\");\r\n\r\n//First we create an instance of a inference algorithm. In this case, we use \r\n//the ImportanceSampling class.\r\nInferenceAlgorithm inferenceAlgorithm = new ImportanceSampling();\r\n\r\n//Then, we set the BN model\r\ninferenceAlgorithm.setModel(bn);\r\n\r\n//If exists, we also set the evidence.\r\nAssignment assignment = new HashMapAssignment(1);\r\nassignment.setValue(varW,0);\r\ninferenceAlgorithm.setEvidence(assignment);\r\n\r\n//We can also set to be run in parallel on multicore CPUs\r\ninferenceAlgorithm.setParallelMode(true);\r\n\r\n//Then we run inference\r\ninferenceAlgorithm.runInference();\r\n\r\n//Then we query the posterior of\r\nSystem.out.println(\"P(Mout|W=0) = \" + inferenceAlgorithm.getPosterior(varMout));\r\n\r\n//Or some more refined queries\r\nSystem.out.println(\"P(0.7<Mout<6.59 | W=0) = \" + \r\n inferenceAlgorithm.getExpectedValue(varMout, v -> (0.7 < v && v < 6.59) ? 1.0 : 0.0 ));\r\n\r\n//We can also compute the probability of the evidence\r\nSystem.out.println(\"P(W=0) = \" + Math.exp(inferenceAlgorithm.getLogProbabilityOfEvidence()));\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n## Dynamic Learning Algorithms <a name=\"dynamiclearningexample\"></a>\r\n### Maximum Likelihood <a name=\"mlexample\"></a>\r\n\r\n\r\nThis other example shows how to learn incrementally the parameters of a Bayesian network using data batches,\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = \r\n                  DataStreamLoader.openFromFile(\"datasets/WasteIncineratorSample.arff\");\r\n\r\n//We create a ParameterLearningAlgorithm object with the MaximumLikehood builder\r\nParameterLearningAlgorithm parameterLearningAlgorithm = new ParallelMaximumLikelihood();\r\n\r\n//We fix the DAG structure\r\nparameterLearningAlgorithm.setDAG(getNaiveBayesStructure(data,0));\r\n\r\n//We should invoke this method before processing any data\r\nparameterLearningAlgorithm.initLearning();\r\n\r\n\r\n//Then we show how we can perform parameter learnig by a sequential updating of data batches.\r\nfor (DataOnMemory<DataInstance> batch : data.iterableOverBatches(100)){\r\n    parameterLearningAlgorithm.updateModel(batch);\r\n}\r\n\r\n//And we get the model\r\nBayesianNetwork bnModel = parameterLearningAlgorithm.getLearntBayesianNetwork();\r\n\r\n//We print the model\r\nSystem.out.println(bnModel.toString());\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Parallel Maximum Likelihood <a name=\"dynamicpmlexample\"></a>\r\n\r\nThis example shows how to learn in parallel the parameters of a Bayesian network from a stream of data using maximum likelihood.\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = \r\n           DataStreamLoader.openFromFile(\"datasets/syntheticData.arff\");\r\n\r\n//We create a MaximumLikelihood object with the MaximumLikehood builder\r\nMaximumLikelihood parameterLearningAlgorithm = new MaximumLikelihood();\r\n\r\n//We activate the parallel mode.\r\nparameterLearningAlgorithm.setParallelMode(true);\r\n\r\n//We fix the DAG structure\r\nparameterLearningAlgorithm.setDAG(getNaiveBayesStructure(data,0));\r\n\r\n//We set the batch size which will be employed to learn the model in parallel\r\nparameterLearningAlgorithm.setBatchSize(100);\r\n\r\n//We set the data which is going to be used for leaning the parameters\r\nparameterLearningAlgorithm.setDataStream(data);\r\n\r\n//We perform the learning\r\nparameterLearningAlgorithm.runLearning();\r\n\r\n//And we get the model\r\nBayesianNetwork bnModel = parameterLearningAlgorithm.getLearntBayesianNetwork();\r\n\r\n//We print the model\r\nSystem.out.println(bnModel.toString());\r\n```\r\n\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Streaming Variational Bayes <a name=\"dynamicsvbexample\"></a>\r\n\r\nThis example shows how to learn incrementally the parameters of a Bayesian network from a stream of data with a Bayesian approach using the following algorithm,\r\n\r\n>Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., \\& Jordan, M. I. (2013). Streaming variational Bayes. \r\nIn Advances in Neural Information Processing Systems (pp. 1727-1735).\r\n\r\nIn this second example we show a alternative implementation which explicitly updates the model by batches by using the class *SVB*.\r\n\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = \r\n                      DataStreamLoader.openFromFile(\"datasets/WasteIncineratorSample.arff\");\r\n\r\n//We create a StreamingVariationalBayesVMP object\r\nStreamingVariationalBayesVMP parameterLearningAlgorithm = new StreamingVariationalBayesVMP();\r\n\r\n//We fix the DAG structure, which is a Naive Bayes with a \r\n//global latent binary variable\r\nparameterLearningAlgorithm.setDAG(getHiddenNaiveBayesStructure(data));\r\n\r\n//We fix the size of the window, which must be equal to the size of the data batches \r\n//we use for learning\r\nparameterLearningAlgorithm.setWindowsSize(5);\r\n\r\n//We can activate the output\r\nparameterLearningAlgorithm.setOutput(true);\r\n\r\n//We should invoke this method before processing any data\r\nparameterLearningAlgorithm.initLearning();\r\n\r\n//Then we show how we can perform parameter learnig by a sequential updating of \r\n//data batches.\r\nfor (DataOnMemory<DataInstance> batch : data.iterableOverBatches(5)){\r\n    parameterLearningAlgorithm.updateModel(batch);\r\n}\r\n\r\n//And we get the model\r\nBayesianNetwork bnModel = parameterLearningAlgorithm.getLearntBayesianNetwork();\r\n\r\n//We print the model\r\nSystem.out.println(bnModel.toString());\r\n```\r\n\r\n[[Back to Top]](#documentation)\r\n\r\n### Parallel Streaming Variational Bayes <a name=\"dynamicpsvbexample\"></a>\r\n\r\nThis example shows how to learn in the parameters of a Bayesian network from a stream of data with a Bayesian\r\napproach using the parallel version  of the SVB algorithm, \r\n\r\n>Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., \\& Jordan, M. I. (2013). Streaming variational Bayes. \r\nIn Advances in Neural Information Processing Systems (pp. 1727-1735).\r\n\r\n```java\r\n//We can open the data stream using the static class DataStreamLoader\r\nDataStream<DataInstance> data = \r\n                   DataStreamLoader.openFromFile(\"datasets/WasteIncineratorSample.arff\");\r\n\r\n//We create a ParallelSVB object\r\nParallelSVB parameterLearningAlgorithm = new ParallelSVB();\r\n\r\n//We fix the number of cores we want to exploit\r\nparameterLearningAlgorithm.setNCores(4);\r\n\r\n//We fix the DAG structure, which is a Naive Bayes with a \r\n//global latent binary variable\r\nparameterLearningAlgorithm.setDAG(StreamingVMPExample.getHiddenNaiveBayesStructure(data));\r\n\r\n\r\n//We fix the size of the window\r\nparameterLearningAlgorithm.getSVBEngine().setWindowsSize(100);\r\n\r\n//We can activate the output\r\nparameterLearningAlgorithm.setOutput(true);\r\n\r\n//We set the data which is going to be used for leaning the parameters\r\nparameterLearningAlgorithm.setDataStream(data);\r\n\r\n//We perform the learning\r\nparameterLearningAlgorithm.runLearning();\r\n\r\n//And we get the model\r\nBayesianNetwork bnModel = parameterLearningAlgorithm.getLearntBayesianNetwork();\r\n\r\n//We print the model\r\nSystem.out.println(bnModel.toString());\r\n```\r\n\r\n[[Back to Top]](#documentation)","google":"UA-66233470-1","note":"Don't delete this file! It's used internally to help with page regeneration."}