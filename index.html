<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>AMIDST Toolbox 1.0 by amidst</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">AMIDST Toolbox 1.0</h1>
      <h2 class="project-tagline">A Java Library for Analysis of MassIve Data Streams using Probabilistic Graphical Models</h2>
      <a href="https://github.com/amidst/toolbox" class="btn">View on GitHub</a>
      <a href="https://github.com/amidst/toolbox/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/amidst/toolbox/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="scope" class="anchor" href="#scope" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scope</h1>

<p>This toolbox offers a collection of scalable and parallel algorithms for inference and learning of hybrid Bayesian networks from streaming data. For example, AMIDST provides parallel multi-core implementations for Bayesian parameter learning, using streaming variational Bayes and variational message passing. Additionally, AMIDST efficiently leverages existing functionalities and algorithms by interfacing to existing software tools such as <a href="https://www.r-project.org/">R</a>, <a href="http://www.hugin.com">Hugin</a> and <a href="http://moa.cms.waikato.ac.nz">MOA</a>. AMIDST is an open source toolbox written in Java and is available under the Apache Software License 2.0.</p>

<p>In the next figure we show a taxonomy of relevant data mining tools dealing with PGMs and data streams. To the best of our knowledge, there is no other software for mining data streams based on PGMs, most of the existing softwares based on PGMs are only focused on mining stationary data sets. Hence, the main goal of AMIDST is to fill this gap and produce a significant contribution within the areas of PGMs and mining streaming data.</p>

<p align="center">
<img title="Taxonomy" src="https://github.com/amidst/toolbox/blob/master/doc/Taxonomy.png?raw=true" width="400">
</p>

<h1>
<a id="scalability" class="anchor" href="#scalability" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scalability</h1>

<p>Scalability is a main concern for the AMIDST toolbox. As mentioned before, we exploit Java 8 functional programming style to provide parallel implementations of most of our algorithms. If more computation capacity is needed to process data streams, AMIDST users can also use more CPU cores. As an example, the following figure shows how the data processing capacity of our toolbox increases with the number of cores when learning a hybrid BN model with latent variables using the AMIDST's learning engine. More precisely we learn a PGM model with multinomial (blue nodes) and Gaussian (green nodes) variables, some of them are latent, non observable, variables (dashed nodes). As can be seen, using our variational learning engine AMIDST toolbox is able to process data in the order of gigabytes per hour depending on the number of available cores with large and complex PGMs with latent variables.</p>

<p align="center">
<img src="https://github.com/amidst/toolbox/blob/master/doc/Scalability.png?raw=true" width="800">
</p>

<h1>
<a id="documentation" class="anchor" href="#documentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Documentation<a name="documentation"></a>
</h1>

<ol>
<li>
<a href="#functionalities">Toolbox Functionalities</a>

<ul>
<li><a href="#datastreams">Data Streams</a></li>
<li><a href="#pgms">Probabilistic Graphical Models</a></li>
<li><a href="#inference">Inference Engine</a></li>
<li><a href="#learning">Learning Engine</a></li>
<li><a href="#conceptdrift">Concept Drift</a></li>
<li><a href="#librarylinks">Links to MOA, Hugin and R</a></li>
</ul>
</li>
<li>
<a href="#architecture">Toolbox Architecture</a>

<ul>
<li><a href="#description">Description</a></li>
<li><a href="#java8">Java 8 Integration: Lambdas, streams, and functional-sytle programming</a></li>
<li>
<a href="#installation">Installing AMIDST Toolbox</a>

<ul>
<li><a href="#installmoa">Installing MOALink</a></li>
<li><a href="#installhugin">Installing HuginLink</a></li>
</ul>
</li>
<li><a href="#compilation">Compiling &amp; Running from the command line</a></li>
</ul>
</li>
<li><a href="#extension">Extending AMIDST with Github Fork &amp; Pull scheme</a></li>
<li>
<a href="#examples">Code Examples</a>

<ul>
<li><a href="#datastreamsexample">Data Streams</a></li>
<li><a href="#variablesexample">Random Variables</a></li>
<li>
<a href="#bnexample">Bayesian Networks</a>

<ul>
<li><a href="#bnnohiddenexample">Creating Bayesian Networks with no hidden variables</a></li>
<li><a href="#bnhiddenexample">Creating Bayesian Networks with hidden variables</a></li>
<li><a href="#bnmodifyexample">Modifying Bayesian Networks</a></li>
</ul>
</li>
<li>
<a href="#ioexample">I/O Functionality</a>

<ul>
<li><a href="#iodatastreamsexample">I/O of Data Streams</a></li>
<li><a href="#iobnsexample">I/O of Bayesian Networks</a></li>
</ul>
</li>
<li>
<a href="#inferenceexample">Inference Algorithms</a>

<ul>
<li><a href="#inferenceengingeexample">The Inference Engine</a></li>
<li><a href="#vmpexample">Variational Message Passing</a></li>
<li><a href="#isexample">Importance Sampling</a></li>
</ul>
</li>
<li>
<a href="#learningexample">Learning Algorithms</a>

<ul>
<li><a href="#mlexample">Maximum Likelihood</a></li>
<li><a href="#pmlexample">Parallel Maximum Likelihood</a></li>
<li><a href="#svbexample">Streaming Variational Bayes</a></li>
<li><a href="#psvbexample">Parallel Streaming Variational Bayes</a></li>
</ul>
</li>
<li>
<a href="#conceptdriftexample">Concept Drift Methods</a>

<ul>
<li><a href="#mlfadingexample">Maximum Likelihood with Fading</a></li>
<li><a href="#svbfadingexample">Streaming Variational Bayes with Fading</a></li>
<li><a href="#nbconceptdriftexample">Naive Bayes with Virtual Concept Drift Detection</a></li>
</ul>
</li>
<li>
<a href="#huginglinkexample">HuginLink</a>

<ul>
<li><a href="#huginglinkconversionexample">Models conversion between AMIDST and Hugin</a></li>
<li><a href="#huginglinkioexample">I/O of Bayesian Networks with Hugin net format</a></li>
<li><a href="#huginglinkinferenceexample">Invoking Hugin's inference engine</a></li>
</ul>
</li>
<li>
<a href="#moalinkexample">MoaLink</a>

<ul>
<li><a href="#moalinkclassifiersexample">AMIDST Classifiers from MOA</a></li>
<li><a href="#moalinkregressionsexample">AMIDST Regression from MOA</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="http://amidst.github.io/toolbox/javadoc/index.html">API Java Doc</a></li>
</ol>

<h2>
<a id="toolbox-functionalities" class="anchor" href="#toolbox-functionalities" aria-hidden="true"><span class="octicon octicon-link"></span></a>Toolbox Functionalities<a name="functionalities"></a>
</h2>

<p>The AMIDST is an open source Java 8 toolbox that makes use of functional programming style to provide parallel processing on mutli-core CPUs \citep{CIM2015}. AMIDST provides a collection of functionalities and algorithms for learning hybrid Bayesian networks from streaming data. In what follows, we describe the main functionalities that AMIDST toolbox supplies.</p>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="data-streams" class="anchor" href="#data-streams" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Streams<a name="datastreams"></a>
</h3>

<p>AMIDST provides parallel processing built-in functionalities for dealing with streaming data \citep{CIM2015}. It is possible to make several passes over the data samples if the stream can be stored on disk, otherwise the samples are discarded after being processed. The data format supported by AMIDST is Weka's ARFF (Attribute-Relation File Format) \citep{Hall2009}.</p>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="probabilistic-graphical-models" class="anchor" href="#probabilistic-graphical-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Probabilistic Graphical Models<a name="pgms"></a>
</h3>

<p>AMIDST currently includes efficient implementations for representing Bayesian networks. AMIDST supports both discrete and continuous variables, and besides Multionomial, Gaussian and conditional linear Gaussian distributions, it also supports other distributions such as Gamma, Poission, Dirichlet, etc. as far as the final BN can be represented as a \textit{conjugate-exponential family model} \citep{WinnBishop2005}.  Other kind of probabilistic graphical models, such as dynamic BNs, are expected to be included in this toolbox.</p>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="inference-engine" class="anchor" href="#inference-engine" aria-hidden="true"><span class="octicon octicon-link"></span></a>Inference Engine<a name="inference"></a>
</h3>

<p>AMIDST includes the implementation of the \textit{variational message passing} \citep{WinnBishop2005} algorithm, and the parallel implementation of the \textit{importance sampling} \citep{hammersley1964monte,CAEPIA2015} algorithm. It also supports exact inference by interfacing with <a href="http://www.hugin.com">Hugin</a>'s junction tree inference algorithm \citep{Madsen2005Hugin}. </p>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="learning-engine" class="anchor" href="#learning-engine" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learning Engine<a name="learning"></a>
</h3>

<p>In AMIDST, a fully Bayesian approach is pursued, which means that the parameter learning reduces to the task of inference. AMIDST provides a multi-core parallel implementation of the \textit{streaming variational Bayes} algorithm \citep{broderick2013streaming}, using \textit{variational message passing} as underlying inference engine, which can deal with large models with latent variables. When the model does not contain latent variables, an efficient parallel implementation of \textit{maximum likelihood estimation} \citep{mlestimation} can be also used by exploiting an efficient vector-based representation of BNs as \textit{exponential family models} \citep{WinnBishop2005}. For structural learning, AMIDST currently supports standard PC and parallel TAN algorithms by interfacing with <a href="http://www.hugin.com">Hugin</a> \citep{Madsen2005Hugin,Madsen2014}.</p>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="concept-drift" class="anchor" href="#concept-drift" aria-hidden="true"><span class="octicon octicon-link"></span></a>Concept drift<a name="conceptdrift"></a>
</h3>

<p>AMIDST also offers some support for dealing with concept drift while learning BNs from data streams. Firstly, we provide an extension of the \textit{streaming variational Bayes} algorithm \citep{broderick2013streaming} which exponentially down-weights the influence of \textit{old} data samples with the use of a fading factor (TODO). So, models learnt with this approach will be \textit{focused} in most recent data. In addition, AMIDST provides a probabilistic concept drift detector based on the use of latent variables \citep{IDA2015}.</p>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="links-to-moa-hugin-and-r" class="anchor" href="#links-to-moa-hugin-and-r" aria-hidden="true"><span class="octicon octicon-link"></span></a>Links to MOA, Hugin and R<a name="librarylinks"></a>
</h3>

<p>AMIDST leverages existing functionalities and algorithms by interfacing to existing software tools such as <a href="https://www.r-project.org/">R</a>, <a href="http://www.hugin.com">Hugin</a> and <a href="http://moa.cms.waikato.ac.nz">MOA</a> (Massive Online Analysis) \citep{BifetHolmesKirkbyPfahringer10}. This allows to efficiently well established systems and also broaden the AMIDST user-base. </p>

<ul>
<li><p><strong>HuginLink</strong> consists of a set of functionalities implemented to link the AMIDST toolbox with the <a href="http://www.hugin.com">Hugin</a> software \citep{Madsen2005Hugin}. This connection extends AMIDST by providing the main functionalities integrated in <a href="http://www.hugin.com">Hugin</a> software, such as exact inference algorithms and scalable structural learning algorithms \citep{Madsen2014}. </p></li>
<li><p><strong>MoaLink</strong> ensures an easy use of AMDIST functionalities within <a href="http://moa.cms.waikato.ac.nz">MOA</a> \citep{BifetHolmesKirkbyPfahringer10}.  The main idea is that any model deployed in AMIDST can be integrated and evaluated using MOA's graphical user interface. As a proof of concept, \textit{MoaLink} already provides a classification, a regression and a clustering method based on BN models with latent variables. These models are learnt in a streaming fashion using AMIDST learning engine. </p></li>
<li><p><strong>RLink</strong> ....</p></li>
</ul>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="toolbox-code-architecture" class="anchor" href="#toolbox-code-architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Toolbox Code Architecture<a name="architecture"></a>
</h2>

<h3>
<a id="description" class="anchor" href="#description" aria-hidden="true"><span class="octicon octicon-link"></span></a>Description<a name="description"></a>
</h3>

<p>AMIDST toolbox is an open source project under <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache Software License 2.0</a>. It is written in Java and is based on <a href="https://en.wikipedia.org/wiki/Apache_Maven">Apache Maven</a> for building and structuring the project. This toolbox is structured as <a href="http://books.sonatype.com/mvnex-book/reference/multimodule.html">multi-module Maven project</a>. Roughly speaking, a <strong>Maven module</strong> is an independent piece of software with explicit dependencies to other modules in the project and to other external libraries. Each module is placed in independent folders and contains an xml file describing its dependencies. In this current version, the toolbox is composed by the following four modules:</p>

<ul>
<li><p><strong>Core module</strong> contains all the main functionalities of the toolbox. It is placed in the <em>core</em> folder. Go to the <a href="http://amidst.github.io/toolbox/javadoc/index.html">Java Doc</a> for details about the different Java classes. </p></li>
<li><p><strong>Examples module</strong> contains basic code examples showing how to use the main functionalities of the toolbox. It is placed in the <em>examples</em> folder under the root project folder.</p></li>
<li><p><strong>MoaLink module</strong> contains the code needed to use the AMIDST functionality within MOA. It is placed in the <em>moalink</em> folder  under the root project folder.</p></li>
<li><p><strong>HuginLink module</strong> contains the code needed to use <a href="www.hugin.com">Hugin software</a> within AMIDST. It is placed in the <em>huginlink</em> folder under the root project folder. </p></li>
</ul>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="java-8-integration-lambdas-streams-and-functional-sytle-programming" class="anchor" href="#java-8-integration-lambdas-streams-and-functional-sytle-programming" aria-hidden="true"><span class="octicon octicon-link"></span></a>Java 8 Integration: Lambdas, streams, and functional-sytle programming<a name="java8"></a>
</h3>

<p>This toolbox has been specifically designed for using the functional-style features provided by the Java 8 release. This design leverages these new features for developing easy-to-code parallel algorithms on mutli-core CPUs. As commented above, the main scalability properties of this toolbox rely on this functional-style approach introduced in Java 8. Our aim is that future developers can also exploit this specific design of the toolbox for easily developing new methods for dealing with massive data streams using PGMs.  </p>

<p>Our paper <a href="">Probabilistic Graphical Models on Multi-Core CPUs using Java 8</a> provides a deep discussion over the different design issues we had to face and how they were solved using Java 8 functional-style features. </p>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="installing-amidst-toolbox-" class="anchor" href="#installing-amidst-toolbox-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Installing AMIDST Toolbox <a name="installation"></a>
</h3>

<p>The first step is to install <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">Java 8</a>.  We strongly recommend <a href="http://www.jetbrains.com/idea/download/">IntelliJ</a> as IDE tool because it has direct support of Maven and Github. </p>

<p>Now, we detail two different installation settings based on Maven and IntelliJ. The first installation settings is for those who just want to use the AMIDST toolbox and do not plan to make contributions/extensions to this open software project. The second settings details how to proceed to be a contributor of this project. </p>

<ul>
<li>
<p><strong>Using AMIDST</strong> simply requires to create a new <strong>Maven Project</strong> using IntelliJ where your code will be placed. Then edit the file "pom.xml" and add the following lines referring to the link the AMIDST jar library inside the dependencies plugin (follow this <a href="http://books.sonatype.com/mvnex-book/reference/customizing-sect-add-depend.html">link</a> for further details) and then you are ready to rock. </p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;eu.amidst.toolbox&lt;/groupId&gt;
    &lt;artifactId&gt;AMIDST&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
</li>
<li><p><strong>Contributing to AMDIST</strong> is based on the <a href="https://help.github.com/articles/using-pull-requests/">Fork &amp; Pull</a> collaboration model. Read this <a href="https://guides.github.com/activities/forking/">guide</a> for full details about how to fork a project and make a pull request. Once you have forked the project and make a local copy to you computer, just you can just open with Intellij the project by pointing at the pom file.Further details about how to contribute to this project are given this <a href="#extension">section</a>. </p></li>
</ul>

<p><a href="#documentation">[Back to Top]</a></p>

<h4>
<a id="installing-moalink" class="anchor" href="#installing-moalink" aria-hidden="true"><span class="octicon octicon-link"></span></a>Installing MOALink<a name="installmoa"></a>
</h4>

<p>To use AMIDST functionality within <a href="http://moa.cms.waikato.ac.nz">MOA</a> you just have to run the script \texttt{compileWithDependencies.sh} in the <em>moalink</em> directory. A file <em>moalink-1.0-SNAPSHOT-jar-with-dependencies.jar</em> will be generated in the \texttt{moalink/target} directory. Please place this jar file on your library path for <a href="http://moa.cms.waikato.ac.nz">MOA</a>. </p>

<p>With this jar file we can make use of the different learning and inference algorithms in AMIDST to learn more expressive Bayesian network models for classification, regression and clustering. AMIDST offers the possibility to add latent Gaussian and/or Multinomial variables to a base naive Bayes structure. Normally, the addition of these latent variables should provide classifiers with lower bias and higher variance, that is, more sophisticated classifiers that are able to lean more complex interdependencies in the data, but also more prone to overfit. The user should evaluate the complexity of his/her dataset and choose the number of latent Gaussian variables and states of the multinomial latent variable accordingly.</p>

<p>With the following command, <a href="http://moa.cms.waikato.ac.nz">MOA</a> gui can be invoked e.g (remember to place \texttt{compileWithDependencies.sh} under the lib folder reference in the <em>-cp</em> option):</p>

<pre><code>java -Xmx512m -cp "../lib/*" -javaagent:../lib/sizeofag-1.0.0.jar moa.gui.GUI
</code></pre>

<p>Note that the above example should be slightly adapted to run on a Windows machine: e.g. use \textasciicircum~ instead of \textbackslash~ to escape brackets.</p>

<h4>
<a id="installing-huginlink" class="anchor" href="#installing-huginlink" aria-hidden="true"><span class="octicon octicon-link"></span></a>Installing HuginLink<a name="installhugin"></a>
</h4>

<p>We describe how to install <a href="http://www.hugin.com/productsservices/demo">Hugin Lite</a> which is a freely available demo version of <a href="http://www.hugin.com">Hugin Software</a>. For those with a full license, just follow the same steps. </p>

<p>Once you have installed <a href="http://www.hugin.com/productsservices/demo">Hugin Lite</a>, you will find inside the installation folder another folder called <em>Libraries</em> which contains the library files needed for the installation. Chose the file that fits with your operation system and copy it to the folder <em>./huginlink/huginlib/</em> inside the project root folder. For example, for a MAC OS X with 64 bits architecture the file needed is <em>libhapi82-64.jnilib</em>.
Now, you can invoke the following example by using the script <em>run.sh</em> :</p>

<pre><code>./run.sh eu.amidst.huginlink.examples.inference.HuginInferenceExample
</code></pre>

<p>We notice that ror running any code invoking the Hugin API, you have to provide the following option to the JVM  </p>

<pre><code>-Djava.library.path="./huginlink/huginlib/" 
</code></pre>

<h3>
<a id="compiling--running-from-the-command-line" class="anchor" href="#compiling--running-from-the-command-line" aria-hidden="true"><span class="octicon octicon-link"></span></a>Compiling &amp; Running from the command line<a name="compilation"></a>
</h3>

<ol>
<li><p>Install Maven: <a href="http://maven.apache.org/download.cgi">http://maven.apache.org/download.cgi</a>  (follow specific instructions for your OS).</p></li>
<li><p>Modify the file maven_startup.sh (which you can find in the root project folder) and fix the path of your maven (Line 5) and java installation (Line 9).</p></li>
<li>
<p>Create (or modify if already exists) a file ".profile" or ".bash_profile" in you home directory and add the following line,
which points to file "maven_startup.sh"</p>

<pre><code>source &lt;project-folder&gt;/maven_startup.sh
</code></pre>

<p>Now after restarting the terminal, mvn should work.</p>
</li>
<li><p>The script "compile.sh" (which you can find in the root project folder) just compiles the whole project.</p></li>
<li>
<p>The script "run.sh" (which you can find in the root project folder) should be used to run some class. For example,</p>

<pre><code>./run.sh eu.amidst.core.examples.learning.ParallelMaximumLikelihoodExample
</code></pre>
</li>
</ol>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="extending-amidst-with-github-fork--pull-scheme" class="anchor" href="#extending-amidst-with-github-fork--pull-scheme" aria-hidden="true"><span class="octicon octicon-link"></span></a>Extending AMIDST with Github Fork &amp; Pull scheme<a name="extension"></a>
</h2>

<p>Developers are expected to contribute to this open software following the <a href="https://help.github.com/articles/using-pull-requests/">Fork &amp; Pull</a> collaboration model. Read this <a href="https://guides.github.com/activities/forking/">guide</a> for full details about how to fork a project and make a pull request.</p>

<p>Some of these collaboration approaches should be followed depending the kind of contribution you are aiming at:</p>

<ul>
<li><p><strong>Basic Contributions</strong> encompasses those contributions to the code that do not imply any major change or addition. For example, fixing a bug, adding methods to existing classes, adding new utility classes, etc. This contributions are made through a <a href="https://help.github.com/articles/using-pull-requests/">pull request</a>, which will be examined by the core group of developers of the project. </p></li>
<li><p><strong>Major Extensions</strong> refers to those contributions which aims to be a new functionality of the toolbox. For example, a new inference/learning algorithm, new PGMs, etc. These extensions or new functionalities will be integrated as new Maven modules will be located in the folder <em>extensions</em> under the root project folder. Contributing with a new extension will be based on the following three steps: (i) create a new Maven module using IntelliJ (follow this <a href="https://www.jetbrains.com/idea/help/creating-maven-module.html">link</a> for details); (ii) code your new algorithm inside this module; and (iii) make a <a href="https://help.github.com/articles/using-pull-requests/">pull request</a> to add the new functionality to the project repository. </p></li>
</ul>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="code-examples" class="anchor" href="#code-examples" aria-hidden="true"><span class="octicon octicon-link"></span></a>Code Examples<a name="examples"></a>
</h2>

<h2>
<a id="data-streams-1" class="anchor" href="#data-streams-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Streams<a name="datastreamsexample"></a>
</h2>

<p>In this example we show how to use the main features of a <em>DataStream</em> object. More precisely,  we show six different ways of iterating over the data samples of a <em>DataStream</em> object.</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>datasets/SmallDataSet.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">//Access to the attributes defining the data set</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>Attributes defining the data set<span class="pl-pds">"</span></span>);
<span class="pl-k">for</span> (<span class="pl-smi">Attribute</span> attribute <span class="pl-k">:</span> data<span class="pl-k">.</span>getAttributes()) {
    <span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(attribute<span class="pl-k">.</span>getName());
}
<span class="pl-smi">Attribute</span> attA <span class="pl-k">=</span> data<span class="pl-k">.</span>getAttributes()<span class="pl-k">.</span>getAttributeByName(<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>);

<span class="pl-c">//1. Iterating over samples using a for loop</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>1. Iterating over samples using a for loop<span class="pl-pds">"</span></span>);
<span class="pl-k">for</span> (<span class="pl-smi">DataInstance</span> dataInstance <span class="pl-k">:</span> data) {
    <span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>The value of attribute A for the current data instance is: <span class="pl-pds">"</span></span> <span class="pl-k">+</span>
                                                          dataInstance<span class="pl-k">.</span>getValue(attA));
}


<span class="pl-c">//2. Iterating using streams. We need to restart the data again </span>
<span class="pl-c">//   as a DataStream can only be used once.</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>2. Iterating using streams.<span class="pl-pds">"</span></span>);
data<span class="pl-k">.</span>restart();
data<span class="pl-k">.</span>stream()<span class="pl-k">.</span>forEach(dataInstance <span class="pl-k">-</span><span class="pl-k">&gt;</span>
                <span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>The value of attribute A for the current data <span class="pl-pds">"</span></span><span class="pl-k">+</span>
                                        instance is<span class="pl-k">:</span> <span class="pl-s"><span class="pl-pds">"</span> + dataInstance.getValue(attA))</span>
<span class="pl-s">);</span>
<span class="pl-s"></span>
<span class="pl-s"></span>
<span class="pl-s">//3. Iterating using parallel streams.</span>
<span class="pl-s">System.out.println(<span class="pl-pds">"</span></span><span class="pl-c1">3.</span> <span class="pl-smi">Iterating</span> using parallel streams<span class="pl-k">.</span><span class="pl-s"><span class="pl-pds">"</span>);</span>
<span class="pl-s">data.restart();</span>
<span class="pl-s">data.parallelStream(10).forEach(dataInstance -&gt;</span>
<span class="pl-s">                System.out.println(<span class="pl-pds">"</span></span><span class="pl-smi">The</span> value of attribute <span class="pl-smi">A</span> <span class="pl-k">for</span> the current data <span class="pl-s"><span class="pl-pds">"</span>+</span>
<span class="pl-s">                                        instance is: <span class="pl-pds">"</span></span> <span class="pl-k">+</span> dataInstance<span class="pl-k">.</span>getValue(attA))
);

<span class="pl-c">//4. Iterating over a stream of data batches.</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>4. Iterating over a stream of data batches.<span class="pl-pds">"</span></span>);
data<span class="pl-k">.</span>restart();
data<span class="pl-k">.</span>streamOfBatches(<span class="pl-c1">10</span>)<span class="pl-k">.</span>forEach(batch <span class="pl-k">-</span><span class="pl-k">&gt;</span> {
    <span class="pl-k">for</span> (<span class="pl-smi">DataInstance</span> dataInstance <span class="pl-k">:</span> batch)
                <span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>The value of attribute A for the current data <span class="pl-pds">"</span></span><span class="pl-k">+</span>
                                        instance is<span class="pl-k">:</span> <span class="pl-s"><span class="pl-pds">"</span> + dataInstance.getValue(attA))</span>
<span class="pl-s">});</span>
<span class="pl-s"></span>
<span class="pl-s">//5. Iterating over a parallel stream of data batches.</span>
<span class="pl-s">System.out.println(<span class="pl-pds">"</span></span><span class="pl-c1">5.</span> <span class="pl-smi">Iterating</span> over a parallel stream of data batches<span class="pl-k">.</span><span class="pl-s"><span class="pl-pds">"</span>);</span>
<span class="pl-s">data.restart();</span>
<span class="pl-s">data.parallelStreamOfBatches(10).forEach(batch -&gt; {</span>
<span class="pl-s">    for (DataInstance dataInstance : batch)</span>
<span class="pl-s">                System.out.println(<span class="pl-pds">"</span></span><span class="pl-smi">The</span> value of attribute <span class="pl-smi">A</span> <span class="pl-k">for</span> the current data <span class="pl-s"><span class="pl-pds">"</span>+</span>
<span class="pl-s">                                        instance is: <span class="pl-pds">"</span></span> <span class="pl-k">+</span> dataInstance<span class="pl-k">.</span>getValue(attA))
});


<span class="pl-c">//6. Iterating over data batches using a for loop</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>6. Iterating over data batches using a for loop.<span class="pl-pds">"</span></span>);
<span class="pl-k">for</span> (<span class="pl-k">DataOnMemory&lt;<span class="pl-smi">DataInstance</span>&gt;</span> batch <span class="pl-k">:</span> data<span class="pl-k">.</span>iterableOverBatches(<span class="pl-c1">10</span>)) {
    <span class="pl-k">for</span> (<span class="pl-smi">DataInstance</span> dataInstance <span class="pl-k">:</span> batch)
                <span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>The value of attribute A for the current data <span class="pl-pds">"</span></span><span class="pl-k">+</span>
                                        instance is<span class="pl-k">:</span> <span class="pl-s"><span class="pl-pds">"</span> + dataInstance.getValue(attA))</span>
<span class="pl-s">}</span></pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="random-variables" class="anchor" href="#random-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random Variables<a name="variablesexample"></a>
</h2>

<p>This example show the basic functionality of the classes Variables and Variable.</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We first create an empty Variables object</span>
<span class="pl-smi">Variables</span> variables <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">Variables</span>();

<span class="pl-c">//We invoke the "new" methods of the object Variables to create new variables.</span>
<span class="pl-c">//Now we create a Gaussian variables</span>
<span class="pl-smi">Variable</span> gaussianVar <span class="pl-k">=</span> variables<span class="pl-k">.</span>newGaussianVariable(<span class="pl-s"><span class="pl-pds">"</span>Gaussian<span class="pl-pds">"</span></span>);

<span class="pl-c">//Now we create a Multinomial variable with two states</span>
<span class="pl-smi">Variable</span> multinomialVar <span class="pl-k">=</span> variables<span class="pl-k">.</span>newMultionomialVariable(<span class="pl-s"><span class="pl-pds">"</span>Multinomial<span class="pl-pds">"</span></span>, <span class="pl-c1">2</span>);

<span class="pl-c">//Now we create a Multinomial variable with two states: TRUE and FALSE</span>
<span class="pl-smi">Variable</span> multinomialVar2 <span class="pl-k">=</span> variables<span class="pl-k">.</span>newMultionomialVariable(<span class="pl-s"><span class="pl-pds">"</span>Multinomial2<span class="pl-pds">"</span></span>, 
                                                <span class="pl-smi">Arrays</span><span class="pl-k">.</span>asList(<span class="pl-s"><span class="pl-pds">"</span>TRUE, FALSE<span class="pl-pds">"</span></span>));

<span class="pl-c">//For Multinomial variables we can iterate over their different states</span>
<span class="pl-smi">FiniteStateSpace</span> states <span class="pl-k">=</span> multinomialVar2<span class="pl-k">.</span>getStateSpaceType();
states<span class="pl-k">.</span>getStatesNames()<span class="pl-k">.</span>forEach(<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">::</span>println);

<span class="pl-c">//Variable objects can also be used, for example, to know if one variable </span>
<span class="pl-c">//can be set as parent of some other variable</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>Can a Gaussian variable be parent of Multinomial variable? <span class="pl-pds">"</span></span> <span class="pl-k">+</span>
        (multinomialVar<span class="pl-k">.</span>getDistributionType()<span class="pl-k">.</span>isParentCompatible(gaussianVar)));

<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>Can a Multinomial variable be parent of Gaussian variable? <span class="pl-pds">"</span></span> <span class="pl-k">+</span>
        (gaussianVar<span class="pl-k">.</span>getDistributionType()<span class="pl-k">.</span>isParentCompatible(multinomialVar)));</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="bayesian-networks" class="anchor" href="#bayesian-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bayesian Networks<a name="bnexample"></a>
</h2>

<h3>
<a id="creating-bayesian-networks-with-no-hidden-variables" class="anchor" href="#creating-bayesian-networks-with-no-hidden-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating Bayesian Networks with no hidden variables<a name="bnnohiddenexample"></a>
</h3>

<p>In this example, we take a data set, create a BN and we compute the log-likelihood of all the samples
of this data set. The numbers defining the probability distributions of the BN are randomly fixed.</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>datasets/syntheticData.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">/**</span>
<span class="pl-c"> * 1. Once the data is loaded, we create a random variable for each of the attributes </span>
<span class="pl-c"> * (i.e. data columns) in our data.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 2. StaticVariables is the class for doing that. It takes a list of Attributes and </span>
<span class="pl-c"> * internally creates all the variables. We create the variables using StaticVariables </span>
<span class="pl-c"> * class to guarantee that each variable has a different ID number and make it </span>
<span class="pl-c"> * transparent for the user.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 3. We can extract the Variable objects by using the method </span>
<span class="pl-c"> * getVariableByName();</span>
<span class="pl-c"> */</span>
<span class="pl-smi">Variables</span> variables <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">Variables</span>(data<span class="pl-k">.</span>getAttributes());

<span class="pl-smi">Variable</span> a <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> b <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>B<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> c <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>C<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> d <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>D<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> e <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>E<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> g <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>G<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> h <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>H<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> i <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>I<span class="pl-pds">"</span></span>);

<span class="pl-c">/**</span>
<span class="pl-c"> * 1. Once you have defined your StaticVariables object, the next step is to create</span>
<span class="pl-c"> * a DAG structure over this set of variables.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 2. To add parents to each variable, we first recover the ParentSet object by the </span>
<span class="pl-c"> * method getParentSet(Variable var) and then call the method addParent().</span>
<span class="pl-c"> */</span>
<span class="pl-smi">DAG</span> dag <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">DAG</span>(variables);

dag<span class="pl-k">.</span>getParentSet(e)<span class="pl-k">.</span>addParent(a);
dag<span class="pl-k">.</span>getParentSet(e)<span class="pl-k">.</span>addParent(b);

dag<span class="pl-k">.</span>getParentSet(h)<span class="pl-k">.</span>addParent(a);
dag<span class="pl-k">.</span>getParentSet(h)<span class="pl-k">.</span>addParent(b);

dag<span class="pl-k">.</span>getParentSet(i)<span class="pl-k">.</span>addParent(a);
dag<span class="pl-k">.</span>getParentSet(i)<span class="pl-k">.</span>addParent(b);
dag<span class="pl-k">.</span>getParentSet(i)<span class="pl-k">.</span>addParent(c);
dag<span class="pl-k">.</span>getParentSet(i)<span class="pl-k">.</span>addParent(d);

dag<span class="pl-k">.</span>getParentSet(g)<span class="pl-k">.</span>addParent(c);
dag<span class="pl-k">.</span>getParentSet(g)<span class="pl-k">.</span>addParent(d);

<span class="pl-c">/**</span>
<span class="pl-c"> * 1. We first check if the graph contains cycles.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 2. We print out the created DAG. We can check that everything is as expected.</span>
<span class="pl-c"> */</span>
<span class="pl-k">if</span> (dag<span class="pl-k">.</span>containCycles()) {
    <span class="pl-k">try</span> {
    } <span class="pl-k">catch</span> (<span class="pl-smi">Exception</span> ex) {
        <span class="pl-k">throw</span> <span class="pl-k">new</span> <span class="pl-smi">IllegalArgumentException</span>(ex);
    }
}

<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(dag<span class="pl-k">.</span>toString());


<span class="pl-c">/**</span>
<span class="pl-c"> * 1. We now create the Bayesian network from the previous DAG.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 2. The BN object is created from the DAG. It automatically looks at the </span>
<span class="pl-c"> * distribution type of each variable and their parents to initialize the </span>
<span class="pl-c"> * Distributions objects that are stored inside (i.e. Multinomial, Normal, </span>
<span class="pl-c"> * CLG, etc). The parameters defining these distributions are properly </span>
<span class="pl-c"> * initialized.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 3. The network is printed and we can have look at the kind of </span>
<span class="pl-c"> * distributions stored in the BN object.</span>
<span class="pl-c"> */</span>
<span class="pl-smi">BayesianNetwork</span> bn <span class="pl-k">=</span> <span class="pl-smi">BayesianNetwork</span><span class="pl-k">.</span>newBayesianNetwork(dag);
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bn<span class="pl-k">.</span>toString());


<span class="pl-c">/**</span>
<span class="pl-c"> * 1. We iterate over the data set sample by sample.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 2. For each sample or DataInstance object, we compute the log of the probability </span>
<span class="pl-c"> * that the BN object assigns to this observation.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 3. We accumulate these log-probs and finally we print the log-prob of the data set.</span>
<span class="pl-c"> */</span>
<span class="pl-k">double</span> logProb <span class="pl-k">=</span> <span class="pl-c1">0</span>;
<span class="pl-k">for</span> (<span class="pl-smi">DataInstance</span> instance <span class="pl-k">:</span> data) {
    logProb <span class="pl-k">+=</span> bn<span class="pl-k">.</span>getLogProbabiltyOf(instance);
}
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(logProb);

<span class="pl-smi">BayesianNetworkWriter</span><span class="pl-k">.</span>saveToFile(bn, <span class="pl-s"><span class="pl-pds">"</span>networks/huginStaticBNExample.bn<span class="pl-pds">"</span></span>);</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="creating-bayesian-networks-with-hidden-variables-" class="anchor" href="#creating-bayesian-networks-with-hidden-variables-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating Bayesian Networks with hidden variables <a name="bnhiddenexample"></a>
</h3>

<p>In this example, we simply show how to create a BN model with hidden variables. We simply create a BN for clustering, i.e.,  a naive-Bayes like structure with a single common hidden variable acting as parant of all the observable variables.</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>datasets/syntheticData.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">/**</span>
<span class="pl-c"> * 1. Once the data is loaded, we create a random variable for each of the attributes </span>
<span class="pl-c"> * (i.e. data columns) in our data.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 2. StaticVariables is the class for doing that. It takes a list of Attributes and </span>
<span class="pl-c"> * internally creates all the variables. We create the variables using StaticVariables </span>
<span class="pl-c"> * class to guarantee that each variable has a different ID number and make it </span>
<span class="pl-c"> * transparent for the user.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 3. We can extract the Variable objects by using the method </span>
<span class="pl-c"> * getVariableByName();</span>
<span class="pl-c"> */</span>
<span class="pl-smi">Variables</span> variables <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">Variables</span>(data<span class="pl-k">.</span>getAttributes());

<span class="pl-smi">Variable</span> a <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>A<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> b <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>B<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> c <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>C<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> d <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>D<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> e <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>E<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> g <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>G<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> h <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>H<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> i <span class="pl-k">=</span> variables<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>I<span class="pl-pds">"</span></span>);

<span class="pl-c">/**</span>
<span class="pl-c"> * 1. We create the hidden variable. For doing that we make use of the class </span>
<span class="pl-c"> * VariableBuilder. When a variable is created from an Attribute object, it </span>
<span class="pl-c"> * contains all the information we need (e.g. the name, the type, etc). But </span>
<span class="pl-c"> * hidden variables does not have an associated attribute and, for this </span>
<span class="pl-c"> * reason, we use now this VariableBuilder to provide this information to</span>
<span class="pl-c"> * StaticVariables object.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 2. Using VariableBuilder, we define a variable called HiddenVar, which is not </span>
<span class="pl-c"> * observable (i.e. hidden), its state space is a finite set with two elements, </span>
<span class="pl-c"> * and its distribution type is multinomial.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 3. We finally create the hidden variable using the method "newVariable".</span>
<span class="pl-c"> */</span>

<span class="pl-smi">Variable</span> hidden <span class="pl-k">=</span> variables<span class="pl-k">.</span>newMultionomialVariable(<span class="pl-s"><span class="pl-pds">"</span>HiddenVar<span class="pl-pds">"</span></span>, <span class="pl-smi">Arrays</span><span class="pl-k">.</span>asList(<span class="pl-s"><span class="pl-pds">"</span>TRUE<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>FALSE<span class="pl-pds">"</span></span>));

<span class="pl-c">/**</span>
<span class="pl-c"> * 1. Once we have defined your StaticVariables object, including the hidden </span>
<span class="pl-c"> * variable, the next step is to create a DAG structure over this set of variables.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 2. To add parents to each variable, we first recover the ParentSet object by </span>
<span class="pl-c"> * the method getParentSet(Variable var) and then call the method </span>
<span class="pl-c"> * addParent(Variable var).</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 3. We just put the hidden variable as parent of all the other variables. Following </span>
<span class="pl-c"> * a naive-Bayes like structure.</span>
<span class="pl-c"> */</span>
<span class="pl-smi">DAG</span> dag <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">DAG</span>(variables);

dag<span class="pl-k">.</span>getParentSet(a)<span class="pl-k">.</span>addParent(hidden);
dag<span class="pl-k">.</span>getParentSet(b)<span class="pl-k">.</span>addParent(hidden);
dag<span class="pl-k">.</span>getParentSet(c)<span class="pl-k">.</span>addParent(hidden);
dag<span class="pl-k">.</span>getParentSet(d)<span class="pl-k">.</span>addParent(hidden);
dag<span class="pl-k">.</span>getParentSet(e)<span class="pl-k">.</span>addParent(hidden);
dag<span class="pl-k">.</span>getParentSet(g)<span class="pl-k">.</span>addParent(hidden);
dag<span class="pl-k">.</span>getParentSet(h)<span class="pl-k">.</span>addParent(hidden);
dag<span class="pl-k">.</span>getParentSet(i)<span class="pl-k">.</span>addParent(hidden);

<span class="pl-c">/**</span>
<span class="pl-c"> * We print the graph to see if is properly created.</span>
<span class="pl-c"> */</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(dag<span class="pl-k">.</span>toString());

<span class="pl-c">/**</span>
<span class="pl-c"> * 1. We now create the Bayesian network from the previous DAG.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 2. The BN object is created from the DAG. It automatically looks at the </span>
<span class="pl-c"> * distribution type of each variable and their parents to initialize the </span>
<span class="pl-c"> * Distributions objects that are stored inside (i.e. Multinomial, Normal, </span>
<span class="pl-c"> * CLG, etc). The parameters defining these distributions are properly </span>
<span class="pl-c"> * initialized.</span>
<span class="pl-c"> *</span>
<span class="pl-c"> * 3. The network is printed and we can have look at the kind of </span>
<span class="pl-c"> * distributions stored in the BN object.</span>
<span class="pl-c"> */</span>
<span class="pl-smi">BayesianNetwork</span> bn <span class="pl-k">=</span> <span class="pl-smi">BayesianNetwork</span><span class="pl-k">.</span>newBayesianNetwork(dag);
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bn<span class="pl-k">.</span>toString());

<span class="pl-c">/**</span>
<span class="pl-c"> * Finally the Bayesian network is saved to a file.</span>
<span class="pl-c"> */</span>
<span class="pl-smi">BayesianNetworkWriter</span><span class="pl-k">.</span>saveToFile(bn, <span class="pl-s"><span class="pl-pds">"</span>networks/huginStaticBNHiddenExample.bn<span class="pl-pds">"</span></span>);</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="modifying-bayesian-networks-" class="anchor" href="#modifying-bayesian-networks-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Modifying Bayesian Networks <a name="bnmodifyexample"></a>
</h3>

<p>In this example we show how to access and modify the conditional probabilities of a Bayesian network model.</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We first generate a Bayesian network with one multinomial, one Gaussian variable and one link.</span>
<span class="pl-smi">BayesianNetworkGenerator</span><span class="pl-k">.</span>setNumberOfGaussianVars(<span class="pl-c1">1</span>);
<span class="pl-smi">BayesianNetworkGenerator</span><span class="pl-k">.</span>setNumberOfMultinomialVars(<span class="pl-c1">1</span>,<span class="pl-c1">2</span>);
<span class="pl-smi">BayesianNetworkGenerator</span><span class="pl-k">.</span>setNumberOfLinks(<span class="pl-c1">1</span>);

<span class="pl-smi">BayesianNetwork</span> bn <span class="pl-k">=</span> <span class="pl-smi">BayesianNetworkGenerator</span><span class="pl-k">.</span>generateBayesianNetwork();

<span class="pl-c">//We print the randomly generated Bayesian networks</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bn<span class="pl-k">.</span>toString());

<span class="pl-c">//We first access the variable we are interested in</span>
<span class="pl-smi">Variable</span> multiVar <span class="pl-k">=</span> bn<span class="pl-k">.</span>getStaticVariables()<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>DiscreteVar0<span class="pl-pds">"</span></span>);

<span class="pl-c">//Using the above variable we can get the associated distribution and modify it</span>
<span class="pl-smi">Multinomial</span> multinomial <span class="pl-k">=</span> bn<span class="pl-k">.</span>getConditionalDistribution(multiVar);
multinomial<span class="pl-k">.</span>setProbabilities(<span class="pl-k">new</span> <span class="pl-smi">double</span>[]{<span class="pl-c1">0.2</span>, <span class="pl-c1">0.8</span>});

<span class="pl-c">//Same than before but accessing the another variable</span>
<span class="pl-smi">Variable</span> normalVar <span class="pl-k">=</span> bn<span class="pl-k">.</span>getStaticVariables()<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>GaussianVar0<span class="pl-pds">"</span></span>);

<span class="pl-c">//In this case, the conditional distribtuion is of the type "Normal given Multinomial Parents"</span>
<span class="pl-smi">Normal_MultinomialParents</span> normalMultiDist <span class="pl-k">=</span> bn<span class="pl-k">.</span>getConditionalDistribution(normalVar);
normalMultiDist<span class="pl-k">.</span>getNormal(<span class="pl-c1">0</span>)<span class="pl-k">.</span>setMean(<span class="pl-c1">1.0</span>);
normalMultiDist<span class="pl-k">.</span>getNormal(<span class="pl-c1">0</span>)<span class="pl-k">.</span>setVariance(<span class="pl-c1">1.0</span>);

normalMultiDist<span class="pl-k">.</span>getNormal(<span class="pl-c1">1</span>)<span class="pl-k">.</span>setMean(<span class="pl-c1">0.0</span>);
normalMultiDist<span class="pl-k">.</span>getNormal(<span class="pl-c1">1</span>)<span class="pl-k">.</span>setVariance(<span class="pl-c1">1.0</span>);

<span class="pl-c">//We print modified Bayesian network</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bn<span class="pl-k">.</span>toString());</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="io-functionality-" class="anchor" href="#io-functionality-" aria-hidden="true"><span class="octicon octicon-link"></span></a>I/O Functionality <a name="ioexample"></a>
</h2>

<h3>
<a id="io-of-data-streams-" class="anchor" href="#io-of-data-streams-" aria-hidden="true"><span class="octicon octicon-link"></span></a>I/O of Data Streams <a name="iodatastreamsexample"></a>
</h3>

<p>In this example we show how to load and save data sets from <a href="http://www.cs.waikato.ac.nz/ml/weka/arff.html">.arff</a> files. </p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>datasets/syntheticData.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">//We can save this data set to a new file using the static class DataStreamWriter</span>
<span class="pl-smi">DataStreamWriter</span><span class="pl-k">.</span>writeDataToFile(data, <span class="pl-s"><span class="pl-pds">"</span>datasets/tmp.arff<span class="pl-pds">"</span></span>);</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="io-of-bayesian-networks-" class="anchor" href="#io-of-bayesian-networks-" aria-hidden="true"><span class="octicon octicon-link"></span></a>I/O of Bayesian Networks <a name="iobnsexample"></a>
</h3>

<p>In this example we show how to load and save Bayesian networks models for a binary file with ".bn" extension. In this toolbox Bayesian networks models are saved as serialized objects.</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can load a Bayesian network using the static class BayesianNetworkLoader</span>
<span class="pl-smi">BayesianNetwork</span> bn <span class="pl-k">=</span> <span class="pl-smi">BayesianNetworkLoader</span><span class="pl-k">.</span>loadFromFile(<span class="pl-s"><span class="pl-pds">"</span>./networks/WasteIncinerator.bn<span class="pl-pds">"</span></span>);

<span class="pl-c">//Now we print the loaded model</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bn<span class="pl-k">.</span>toString());

<span class="pl-c">//Now we change the parameters of the model</span>
bn<span class="pl-k">.</span>randomInitialization(<span class="pl-k">new</span> <span class="pl-smi">Random</span>(<span class="pl-c1">0</span>));

<span class="pl-c">//We can save this Bayesian network to using the static class BayesianNetworkWriter</span>
<span class="pl-smi">BayesianNetworkWriter</span><span class="pl-k">.</span>saveToFile(bn, <span class="pl-s"><span class="pl-pds">"</span>networks/tmp.bn<span class="pl-pds">"</span></span>);</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="inference-algorithms-" class="anchor" href="#inference-algorithms-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Inference Algorithms <a name="inferenceexample"></a>
</h2>

<h3>
<a id="the-inference-engine-" class="anchor" href="#the-inference-engine-" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Inference Engine <a name="inferenceengingeexample"></a>
</h3>

<p>This example show how to perform inference in a Bayesian network model using the InferenceEngine static class. This class aims to be a straigthfoward way to perform queries over a Bayesian network model. By the default the \textit{VMP} inference method is invoked.</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We first load the WasteIncinerator bayesian network which has multinomial </span>
<span class="pl-c">//and Gaussian variables.</span>
<span class="pl-smi">BayesianNetwork</span> bn <span class="pl-k">=</span> <span class="pl-smi">BayesianNetworkLoader</span><span class="pl-k">.</span>loadFromFile(<span class="pl-s"><span class="pl-pds">"</span>./networks/WasteIncinerator.bn<span class="pl-pds">"</span></span>);

<span class="pl-c">//We recover the relevant variables for this example: Mout which is normally </span>
<span class="pl-c">//distributed, and W which is multinomial.</span>
<span class="pl-smi">Variable</span> varMout <span class="pl-k">=</span> bn<span class="pl-k">.</span>getStaticVariables()<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>Mout<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> varW <span class="pl-k">=</span> bn<span class="pl-k">.</span>getStaticVariables()<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>W<span class="pl-pds">"</span></span>);

<span class="pl-c">//Set the evidence.</span>
<span class="pl-smi">Assignment</span> assignment <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">HashMapAssignment</span>(<span class="pl-c1">1</span>);
assignment<span class="pl-k">.</span>setValue(varW,<span class="pl-c1">0</span>);

<span class="pl-c">//Then we query the posterior of</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>P(Mout|W=0) = <span class="pl-pds">"</span></span> <span class="pl-k">+</span> <span class="pl-smi">InferenceEngine</span><span class="pl-k">.</span>getPosterior(varMout, bn, assignment));

<span class="pl-c">//Or some more refined queries</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>P(0.7&lt;Mout&lt;6.59 | W=0) = <span class="pl-pds">"</span></span> <span class="pl-k">+</span> 
 <span class="pl-smi">InferenceEngine</span><span class="pl-k">.</span>getExpectedValue(varMout, bn, v <span class="pl-k">-</span><span class="pl-k">&gt;</span> (<span class="pl-c1">0.7</span> <span class="pl-k">&lt;</span> v <span class="pl-k">&amp;&amp;</span> v <span class="pl-k">&lt;</span> <span class="pl-c1">6.59</span>) <span class="pl-k">?</span> <span class="pl-c1">1.0</span> <span class="pl-k">:</span> <span class="pl-c1">0.0</span> ));</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="variational-message-passing-" class="anchor" href="#variational-message-passing-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Variational Message Passing <a name="vmpexample"></a>
</h3>

<p>This example we show how to perform inference on a general Bayesian network using the Variational Message Passing (VMP)
algorithm detailed in</p>

<blockquote>
<p>Winn, J. M., Bishop, C. M. (2005). Variational message passing. In Journal of Machine Learning Research (pp. 661-694).</p>
</blockquote>

<div class="highlight highlight-java"><pre><span class="pl-c">//We first load the WasteIncinerator bayesian network which has multinomial </span>
<span class="pl-c">//and Gaussian variables.</span>
<span class="pl-smi">BayesianNetwork</span> bn <span class="pl-k">=</span> <span class="pl-smi">BayesianNetworkLoader</span><span class="pl-k">.</span>loadFromFile(<span class="pl-s"><span class="pl-pds">"</span>./networks/WasteIncinerator.bn<span class="pl-pds">"</span></span>);

<span class="pl-c">//We recover the relevant variables for this example: Mout which is normally </span>
<span class="pl-c">//distributed, and W which is multinomial.</span>
<span class="pl-smi">Variable</span> varMout <span class="pl-k">=</span> bn<span class="pl-k">.</span>getStaticVariables()<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>Mout<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> varW <span class="pl-k">=</span> bn<span class="pl-k">.</span>getStaticVariables()<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>W<span class="pl-pds">"</span></span>);

<span class="pl-c">//First we create an instance of a inference algorithm. In this case, we use </span>
<span class="pl-c">//the VMP class.</span>
<span class="pl-smi">InferenceAlgorithm</span> inferenceAlgorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">VMP</span>();

<span class="pl-c">//Then, we set the BN model</span>
inferenceAlgorithm<span class="pl-k">.</span>setModel(bn);

<span class="pl-c">//If exists, we also set the evidence.</span>
<span class="pl-smi">Assignment</span> assignment <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">HashMapAssignment</span>(<span class="pl-c1">1</span>);
assignment<span class="pl-k">.</span>setValue(varW,<span class="pl-c1">0</span>);
inferenceAlgorithm<span class="pl-k">.</span>setEvidence(assignment);

<span class="pl-c">//Then we run inference</span>
inferenceAlgorithm<span class="pl-k">.</span>runInference();

<span class="pl-c">//Then we query the posterior of</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>P(Mout|W=0) = <span class="pl-pds">"</span></span> <span class="pl-k">+</span> inferenceAlgorithm<span class="pl-k">.</span>getPosterior(varMout));

<span class="pl-c">//Or some more refined queries</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>P(0.7&lt;Mout&lt;6.59 | W=0) = <span class="pl-pds">"</span></span> <span class="pl-k">+</span> 
 inferenceAlgorithm<span class="pl-k">.</span>getExpectedValue(varMout, v <span class="pl-k">-</span><span class="pl-k">&gt;</span> (<span class="pl-c1">0.7</span> <span class="pl-k">&lt;</span> v <span class="pl-k">&amp;&amp;</span> v <span class="pl-k">&lt;</span> <span class="pl-c1">6.59</span>) <span class="pl-k">?</span> <span class="pl-c1">1.0</span> <span class="pl-k">:</span> <span class="pl-c1">0.0</span> ));

<span class="pl-c">//We can also compute the probability of the evidence</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>P(W=0) = <span class="pl-pds">"</span></span> <span class="pl-k">+</span> <span class="pl-smi">Math</span><span class="pl-k">.</span>exp(inferenceAlgorithm<span class="pl-k">.</span>getLogProbabilityOfEvidence()));</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="importance-sampling-" class="anchor" href="#importance-sampling-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Importance Sampling <a name="isexample"></a>
</h3>

<p>This example we show how to perform inference on a general Bayesian network using an importance sampling
algorithm detailed in</p>

<blockquote>
<p>Fung, R., Chang, K. C. (2013). Weighing and integrating evidence for stochastic simulation in Bayesian networks. arXiv preprint arXiv:1304.1504.</p>
</blockquote>

<div class="highlight highlight-java"><pre><span class="pl-c">//We first load the WasteIncinerator bayesian network which has multinomial </span>
<span class="pl-c">//and Gaussian variables.</span>
<span class="pl-smi">BayesianNetwork</span> bn <span class="pl-k">=</span> <span class="pl-smi">BayesianNetworkLoader</span><span class="pl-k">.</span>loadFromFile(<span class="pl-s"><span class="pl-pds">"</span>./networks/WasteIncinerator.bn<span class="pl-pds">"</span></span>);

<span class="pl-c">//We recover the relevant variables for this example: Mout which is normally </span>
<span class="pl-c">//distributed, and W which is multinomial.</span>
<span class="pl-smi">Variable</span> varMout <span class="pl-k">=</span> bn<span class="pl-k">.</span>getStaticVariables()<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>Mout<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> varW <span class="pl-k">=</span> bn<span class="pl-k">.</span>getStaticVariables()<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>W<span class="pl-pds">"</span></span>);

<span class="pl-c">//First we create an instance of a inference algorithm. In this case, we use </span>
<span class="pl-c">//the ImportanceSampling class.</span>
<span class="pl-smi">InferenceAlgorithm</span> inferenceAlgorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">ImportanceSampling</span>();

<span class="pl-c">//Then, we set the BN model</span>
inferenceAlgorithm<span class="pl-k">.</span>setModel(bn);

<span class="pl-c">//If exists, we also set the evidence.</span>
<span class="pl-smi">Assignment</span> assignment <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">HashMapAssignment</span>(<span class="pl-c1">1</span>);
assignment<span class="pl-k">.</span>setValue(varW,<span class="pl-c1">0</span>);
inferenceAlgorithm<span class="pl-k">.</span>setEvidence(assignment);

<span class="pl-c">//We can also set to be run in parallel on multicore CPUs</span>
inferenceAlgorithm<span class="pl-k">.</span>setParallelMode(<span class="pl-c1">true</span>);

<span class="pl-c">//Then we run inference</span>
inferenceAlgorithm<span class="pl-k">.</span>runInference();

<span class="pl-c">//Then we query the posterior of</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>P(Mout|W=0) = <span class="pl-pds">"</span></span> <span class="pl-k">+</span> inferenceAlgorithm<span class="pl-k">.</span>getPosterior(varMout));

<span class="pl-c">//Or some more refined queries</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>P(0.7&lt;Mout&lt;6.59 | W=0) = <span class="pl-pds">"</span></span> <span class="pl-k">+</span> 
 inferenceAlgorithm<span class="pl-k">.</span>getExpectedValue(varMout, v <span class="pl-k">-</span><span class="pl-k">&gt;</span> (<span class="pl-c1">0.7</span> <span class="pl-k">&lt;</span> v <span class="pl-k">&amp;&amp;</span> v <span class="pl-k">&lt;</span> <span class="pl-c1">6.59</span>) <span class="pl-k">?</span> <span class="pl-c1">1.0</span> <span class="pl-k">:</span> <span class="pl-c1">0.0</span> ));

<span class="pl-c">//We can also compute the probability of the evidence</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>P(W=0) = <span class="pl-pds">"</span></span> <span class="pl-k">+</span> <span class="pl-smi">Math</span><span class="pl-k">.</span>exp(inferenceAlgorithm<span class="pl-k">.</span>getLogProbabilityOfEvidence()));</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="learning-algorithms-" class="anchor" href="#learning-algorithms-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learning Algorithms <a name="learningexample"></a>
</h2>

<h3>
<a id="maximum-likelihood-" class="anchor" href="#maximum-likelihood-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Maximum Likelihood <a name="mlexample"></a>
</h3>

<p>This other example shows how to learn incrementally the parameters of a Bayesian network using data batches,</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> 
                  <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>datasets/WasteIncineratorSample.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">//We create a ParameterLearningAlgorithm object with the MaximumLikehood builder</span>
<span class="pl-smi">ParameterLearningAlgorithm</span> parameterLearningAlgorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">ParallelMaximumLikelihood</span>();

<span class="pl-c">//We fix the DAG structure</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setDAG(getNaiveBayesStructure(data,<span class="pl-c1">0</span>));

<span class="pl-c">//We should invoke this method before processing any data</span>
parameterLearningAlgorithm<span class="pl-k">.</span>initLearning();


<span class="pl-c">//Then we show how we can perform parameter learnig by a sequential updating of data batches.</span>
<span class="pl-k">for</span> (<span class="pl-k">DataOnMemory&lt;<span class="pl-smi">DataInstance</span>&gt;</span> batch <span class="pl-k">:</span> data<span class="pl-k">.</span>iterableOverBatches(<span class="pl-c1">100</span>)){
    parameterLearningAlgorithm<span class="pl-k">.</span>updateModel(batch);
}

<span class="pl-c">//And we get the model</span>
<span class="pl-smi">BayesianNetwork</span> bnModel <span class="pl-k">=</span> parameterLearningAlgorithm<span class="pl-k">.</span>getLearntBayesianNetwork();

<span class="pl-c">//We print the model</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bnModel<span class="pl-k">.</span>toString());</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="parallel-maximum-likelihood-" class="anchor" href="#parallel-maximum-likelihood-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parallel Maximum Likelihood <a name="pmlexample"></a>
</h3>

<p>This example shows how to learn in parallel the parameters of a Bayesian network from a stream of data using maximum likelihood.</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> 
           <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>datasets/syntheticData.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">//We create a MaximumLikelihood object with the MaximumLikehood builder</span>
<span class="pl-smi">MaximumLikelihood</span> parameterLearningAlgorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">MaximumLikelihood</span>();

<span class="pl-c">//We activate the parallel mode.</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setParallelMode(<span class="pl-c1">true</span>);

<span class="pl-c">//We fix the DAG structure</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setDAG(getNaiveBayesStructure(data,<span class="pl-c1">0</span>));

<span class="pl-c">//We set the batch size which will be employed to learn the model in parallel</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setBatchSize(<span class="pl-c1">100</span>);

<span class="pl-c">//We set the data which is going to be used for leaning the parameters</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setDataStream(data);

<span class="pl-c">//We perform the learning</span>
parameterLearningAlgorithm<span class="pl-k">.</span>runLearning();

<span class="pl-c">//And we get the model</span>
<span class="pl-smi">BayesianNetwork</span> bnModel <span class="pl-k">=</span> parameterLearningAlgorithm<span class="pl-k">.</span>getLearntBayesianNetwork();

<span class="pl-c">//We print the model</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bnModel<span class="pl-k">.</span>toString());</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="streaming-variational-bayes-" class="anchor" href="#streaming-variational-bayes-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Streaming Variational Bayes <a name="svbexample"></a>
</h3>

<p>This example shows how to learn incrementally the parameters of a Bayesian network from a stream of data with a Bayesian approach using the following algorithm,</p>

<blockquote>
<p>Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., &amp; Jordan, M. I. (2013). Streaming variational Bayes. 
In Advances in Neural Information Processing Systems (pp. 1727-1735).</p>
</blockquote>

<p>In this second example we show a alternative implementation which explicitly updates the model by batches by using the class <em>SVB</em>.</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> 
                      <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>datasets/WasteIncineratorSample.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">//We create a StreamingVariationalBayesVMP object</span>
<span class="pl-smi">StreamingVariationalBayesVMP</span> parameterLearningAlgorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">StreamingVariationalBayesVMP</span>();

<span class="pl-c">//We fix the DAG structure, which is a Naive Bayes with a </span>
<span class="pl-c">//global latent binary variable</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setDAG(getHiddenNaiveBayesStructure(data));

<span class="pl-c">//We fix the size of the window, which must be equal to the size of the data batches </span>
<span class="pl-c">//we use for learning</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setWindowsSize(<span class="pl-c1">5</span>);

<span class="pl-c">//We can activate the output</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setOutput(<span class="pl-c1">true</span>);

<span class="pl-c">//We should invoke this method before processing any data</span>
parameterLearningAlgorithm<span class="pl-k">.</span>initLearning();

<span class="pl-c">//Then we show how we can perform parameter learnig by a sequential updating of </span>
<span class="pl-c">//data batches.</span>
<span class="pl-k">for</span> (<span class="pl-k">DataOnMemory&lt;<span class="pl-smi">DataInstance</span>&gt;</span> batch <span class="pl-k">:</span> data<span class="pl-k">.</span>iterableOverBatches(<span class="pl-c1">5</span>)){
    parameterLearningAlgorithm<span class="pl-k">.</span>updateModel(batch);
}

<span class="pl-c">//And we get the model</span>
<span class="pl-smi">BayesianNetwork</span> bnModel <span class="pl-k">=</span> parameterLearningAlgorithm<span class="pl-k">.</span>getLearntBayesianNetwork();

<span class="pl-c">//We print the model</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bnModel<span class="pl-k">.</span>toString());</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="parallel-streaming-variational-bayes-" class="anchor" href="#parallel-streaming-variational-bayes-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parallel Streaming Variational Bayes <a name="psvbexample"></a>
</h3>

<p>This example shows how to learn in the parameters of a Bayesian network from a stream of data with a Bayesian
approach using the parallel version  of the SVB algorithm, </p>

<blockquote>
<p>Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., &amp; Jordan, M. I. (2013). Streaming variational Bayes. 
In Advances in Neural Information Processing Systems (pp. 1727-1735).</p>
</blockquote>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> 
                   <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>datasets/WasteIncineratorSample.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">//We create a ParallelSVB object</span>
<span class="pl-smi">ParallelSVB</span> parameterLearningAlgorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">ParallelSVB</span>();

<span class="pl-c">//We fix the number of cores we want to exploit</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setNCores(<span class="pl-c1">4</span>);

<span class="pl-c">//We fix the DAG structure, which is a Naive Bayes with a </span>
<span class="pl-c">//global latent binary variable</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setDAG(<span class="pl-smi">StreamingVMPExample</span><span class="pl-k">.</span>getHiddenNaiveBayesStructure(data));


<span class="pl-c">//We fix the size of the window</span>
parameterLearningAlgorithm<span class="pl-k">.</span>getSVBEngine()<span class="pl-k">.</span>setWindowsSize(<span class="pl-c1">100</span>);

<span class="pl-c">//We can activate the output</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setOutput(<span class="pl-c1">true</span>);

<span class="pl-c">//We set the data which is going to be used for leaning the parameters</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setDataStream(data);

<span class="pl-c">//We perform the learning</span>
parameterLearningAlgorithm<span class="pl-k">.</span>runLearning();

<span class="pl-c">//And we get the model</span>
<span class="pl-smi">BayesianNetwork</span> bnModel <span class="pl-k">=</span> parameterLearningAlgorithm<span class="pl-k">.</span>getLearntBayesianNetwork();

<span class="pl-c">//We print the model</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bnModel<span class="pl-k">.</span>toString());</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="concept-drift-methods-" class="anchor" href="#concept-drift-methods-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Concept Drift Methods <a name="conceptdriftexample"></a>
</h2>

<h3>
<a id="maximum-likelihood-with-fading-" class="anchor" href="#maximum-likelihood-with-fading-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Maximum Likelihood with Fading <a name="mlfadingexample"></a>
</h3>

<p>This example shows how to adaptively learn the parameters of a Bayesian network from a stream of data using exponential forgetting with a given fading factor, directly inspired by the approach presented in</p>

<blockquote>
<p>Olesen, K. G., Lauritzen, S. L., &amp; Jensen, F. V. (1992, July). aHUGIN: A system creating adaptive causal probabilistic networks. In Proceedings of the Eighth international conference on Uncertainty in Artificial Intelligence (pp. 223-229). Morgan Kaufmann Publishers Inc.</p>
</blockquote>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> 
             <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>datasets/WasteIncineratorSample.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">//We create a ParameterLearningAlgorithm object with </span>
<span class="pl-c">//the MaximumLikelihoodFading builder</span>
<span class="pl-smi">MaximumLikelihoodFading</span> parameterLearningAlgorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">MaximumLikelihoodFading</span>();

<span class="pl-c">//We fix the DAG structure</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setDAG(
                <span class="pl-smi">MaximimumLikelihoodByBatchExample</span><span class="pl-k">.</span>getNaiveBayesStructure(data, <span class="pl-c1">0</span>));

<span class="pl-c">//We fix the fading or forgeting factor</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setFadingFactor(<span class="pl-c1">0.9</span>);

<span class="pl-c">//We set the batch size which will be employed to learn the model</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setBatchSize(<span class="pl-c1">100</span>);

<span class="pl-c">//We set the data which is going to be used for leaning the parameters</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setDataStream(data);

<span class="pl-c">//We perform the learning</span>
parameterLearningAlgorithm<span class="pl-k">.</span>runLearning();

<span class="pl-c">//And we get the model</span>
<span class="pl-smi">BayesianNetwork</span> bnModel <span class="pl-k">=</span> parameterLearningAlgorithm<span class="pl-k">.</span>getLearntBayesianNetwork();

<span class="pl-c">//We print the model</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bnModel<span class="pl-k">.</span>toString());</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="streaming-variational-bayes-with-fading-" class="anchor" href="#streaming-variational-bayes-with-fading-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Streaming Variational Bayes with Fading <a name="svbfadingexample"></a>
</h3>

<p>This example shows how to adaptively learn in the parameters of a Bayesian network from a stream of data with a Bayesian approach using a combination of the the following two methods,</p>

<blockquote>
<p>Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., &amp; Jordan, M. I. (2013). Streaming variational Bayes. 
In Advances in Neural Information Processing Systems (pp. 1727-1735).</p>

<p>Olesen, K. G., Lauritzen, S. L., &amp; Jensen, F. V. (1992, July). aHUGIN: A system creating adaptive causal probabilistic networks. In Proceedings of the Eighth international conference on Uncertainty in Artificial Intelligence (pp. 223-229). Morgan Kaufmann Publishers Inc.</p>
</blockquote>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> 
           <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>datasets/WasteIncineratorSample.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">//We create a SVB object</span>
<span class="pl-smi">SVBFading</span> parameterLearningAlgorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">SVBFading</span>();

<span class="pl-c">//We fix the DAG structure</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setDAG(<span class="pl-smi">SVBExample</span><span class="pl-k">.</span>getHiddenNaiveBayesStructure(data));

<span class="pl-c">//We fix the fading or forgeting factor</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setFadingFactor(<span class="pl-c1">0.9</span>);

<span class="pl-c">//We fix the size of the window</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setWindowsSize(<span class="pl-c1">100</span>);

<span class="pl-c">//We can activate the output</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setOutput(<span class="pl-c1">true</span>);

<span class="pl-c">//We set the data which is going to be used for leaning the parameters</span>
parameterLearningAlgorithm<span class="pl-k">.</span>setDataStream(data);

<span class="pl-c">//We perform the learning</span>
parameterLearningAlgorithm<span class="pl-k">.</span>runLearning();

<span class="pl-c">//And we get the model</span>
<span class="pl-smi">BayesianNetwork</span> bnModel <span class="pl-k">=</span> parameterLearningAlgorithm<span class="pl-k">.</span>getLearntBayesianNetwork();

<span class="pl-c">//We print the model</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(bnModel<span class="pl-k">.</span>toString());</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="naive-bayes-with-virtual-concept-drift-detection-" class="anchor" href="#naive-bayes-with-virtual-concept-drift-detection-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Naive Bayes with Virtual Concept Drift Detection <a name="nbconceptdriftexample"></a>
</h3>

<p>This example shows how to use the class NaiveBayesVirtualConceptDriftDetector to run the virtual concept drift detector detailed in</p>

<blockquote>
<p>Borchani et al. Modeling concept drift: A probabilistic graphical model based approach. IDA 2015.</p>
</blockquote>

<div class="highlight highlight-java"><pre><span class="pl-c">//We can open the data stream using the static class DataStreamLoader</span>
<span class="pl-k">DataStream&lt;<span class="pl-smi">DataInstance</span>&gt;</span> data <span class="pl-k">=</span> <span class="pl-smi">DataStreamLoader</span><span class="pl-k">.</span>openFromFile(<span class="pl-s"><span class="pl-pds">"</span>./datasets/DriftSets/sea.arff<span class="pl-pds">"</span></span>);

<span class="pl-c">//We create a NaiveBayesVirtualConceptDriftDetector object</span>
<span class="pl-smi">NaiveBayesVirtualConceptDriftDetector</span> virtualDriftDetector <span class="pl-k">=</span> 
                                        <span class="pl-k">new</span> <span class="pl-smi">NaiveBayesVirtualConceptDriftDetector</span>();

<span class="pl-c">//We set class variable as the last attribute</span>
virtualDriftDetector<span class="pl-k">.</span>setClassIndex(<span class="pl-k">-</span><span class="pl-c1">1</span>);

<span class="pl-c">//We set the data which is going to be used</span>
virtualDriftDetector<span class="pl-k">.</span>setData(data);

<span class="pl-c">//We fix the size of the window</span>
<span class="pl-k">int</span> windowSize <span class="pl-k">=</span> <span class="pl-c1">1000</span>;
virtualDriftDetector<span class="pl-k">.</span>setWindowsSize(windowSize);

<span class="pl-c">//We fix the so-called transition variance</span>
virtualDriftDetector<span class="pl-k">.</span>setTransitionVariance(<span class="pl-c1">0.1</span>);

<span class="pl-c">//We fix the number of global latent variables</span>
virtualDriftDetector<span class="pl-k">.</span>setNumberOfGlobalVars(<span class="pl-c1">1</span>);

<span class="pl-c">//We should invoke this method before processing any data</span>
virtualDriftDetector<span class="pl-k">.</span>initLearning();

<span class="pl-c">//Some prints</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>print(<span class="pl-s"><span class="pl-pds">"</span>Batch<span class="pl-pds">"</span></span>);
<span class="pl-k">for</span> (<span class="pl-smi">Variable</span> hiddenVar <span class="pl-k">:</span> virtualDriftDetector<span class="pl-k">.</span>getHiddenVars()) {
    <span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>print(<span class="pl-s"><span class="pl-pds">"</span><span class="pl-cce">\t</span><span class="pl-pds">"</span></span> <span class="pl-k">+</span> hiddenVar<span class="pl-k">.</span>getName());
}
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println();


<span class="pl-c">//Then we show how we can perform the sequential processing of</span>
<span class="pl-c">// data batches. They must be of the same value than the window</span>
<span class="pl-c">// size parameter set above.</span>
<span class="pl-k">int</span> countBatch <span class="pl-k">=</span> <span class="pl-c1">0</span>;
<span class="pl-k">for</span> (<span class="pl-k">DataOnMemory&lt;<span class="pl-smi">DataInstance</span>&gt;</span> batch <span class="pl-k">:</span> 
                data<span class="pl-k">.</span>iterableOverBatches(windowSize)){

    <span class="pl-c">//We update the model by invoking this method. The output</span>
    <span class="pl-c">// is an array with a value associated</span>
    <span class="pl-c">// to each fo the global hidden variables</span>
    <span class="pl-k">double</span>[] out <span class="pl-k">=</span> virtualDriftDetector<span class="pl-k">.</span>updateModel(batch);

    <span class="pl-c">//We print the output</span>
    <span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>print(countBatch <span class="pl-k">+</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-cce">\t</span><span class="pl-pds">"</span></span>);
    <span class="pl-k">for</span> (<span class="pl-k">int</span> i <span class="pl-k">=</span> <span class="pl-c1">0</span>; i <span class="pl-k">&lt;</span> out<span class="pl-k">.</span>length; i<span class="pl-k">++</span>) {
        <span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>print(out[i]<span class="pl-k">+</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-cce">\t</span><span class="pl-pds">"</span></span>);
    }
    <span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println();
    countBatch<span class="pl-k">++</span>;
}</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="huginlink-" class="anchor" href="#huginlink-" aria-hidden="true"><span class="octicon octicon-link"></span></a>HuginLink <a name="huginglinkexample"></a>
</h2>

<h3>
<a id="models-conversion-between-amidst-and-hugin-" class="anchor" href="#models-conversion-between-amidst-and-hugin-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Models conversion between AMIDST and Hugin <a name="huginglinkconversionexample"></a>
</h3>

<p>This example shows how to use the class BNConverterToAMIDST and BNConverterToHugin to convert a 
Bayesian network models between Hugin and AMIDST formats</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We load from Hugin format</span>
<span class="pl-smi">Domain</span> huginBN <span class="pl-k">=</span> <span class="pl-smi">BNLoaderFromHugin</span><span class="pl-k">.</span>loadFromFile(<span class="pl-s"><span class="pl-pds">"</span>networks/asia.net<span class="pl-pds">"</span></span>);

<span class="pl-c">//Then, it is converted to AMIDST BayesianNetwork object</span>
<span class="pl-smi">BayesianNetwork</span> amidstBN <span class="pl-k">=</span> <span class="pl-smi">BNConverterToAMIDST</span><span class="pl-k">.</span>convertToAmidst(huginBN);

<span class="pl-c">//Then, it is converted to Hugin Bayesian Network object</span>
huginBN <span class="pl-k">=</span> <span class="pl-smi">BNConverterToHugin</span><span class="pl-k">.</span>convertToHugin(amidstBN);

<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(amidstBN<span class="pl-k">.</span>toString());
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(huginBN<span class="pl-k">.</span>toString());</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="io-of-bayesian-networks-with-hugin-net-format-" class="anchor" href="#io-of-bayesian-networks-with-hugin-net-format-" aria-hidden="true"><span class="octicon octicon-link"></span></a>I/O of Bayesian Networks with Hugin net format <a name="huginglinkioexample"></a>
</h3>

<p>This example shows how to use the class BNLoaderFromHugin and BNWriterToHugin classes to load and
write Bayesian networks in Hugin format.</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We load from Hugin format</span>
<span class="pl-smi">Domain</span> huginBN <span class="pl-k">=</span> <span class="pl-smi">BNLoaderFromHugin</span><span class="pl-k">.</span>loadFromFile(<span class="pl-s"><span class="pl-pds">"</span>networks/asia.net<span class="pl-pds">"</span></span>);

<span class="pl-c">//We save a AMIDST BN to Hugin format</span>
<span class="pl-smi">BayesianNetwork</span> amidstBN <span class="pl-k">=</span> <span class="pl-smi">BNConverterToAMIDST</span><span class="pl-k">.</span>convertToAmidst(huginBN);
<span class="pl-smi">BNWriterToHugin</span><span class="pl-k">.</span>saveToHuginFile(amidstBN,<span class="pl-s"><span class="pl-pds">"</span>networks/tmp.net<span class="pl-pds">"</span></span>);</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="invoking-hugins-inference-engine-" class="anchor" href="#invoking-hugins-inference-engine-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Invoking Hugin's inference engine <a name="huginglinkinferenceexample"></a>
</h3>

<p>This example we show how to perform inference using <a href="http://www.hugin.com">Hugin</a> inference engine within the AMIDST toolbox</p>

<div class="highlight highlight-java"><pre><span class="pl-c">//We first load the WasteIncinerator bayesian network </span>
<span class="pl-c">//which has multinomial and Gaussian variables.</span>
<span class="pl-smi">BayesianNetwork</span> bn <span class="pl-k">=</span> <span class="pl-smi">BayesianNetworkLoader</span><span class="pl-k">.</span>loadFromFile(<span class="pl-s"><span class="pl-pds">"</span>./networks/WasteIncinerator.bn<span class="pl-pds">"</span></span>);

<span class="pl-c">//We recover the relevant variables for this example: </span>
<span class="pl-c">//Mout which is normally distributed, and W which is multinomial.</span>
<span class="pl-smi">Variable</span> varMout <span class="pl-k">=</span> bn<span class="pl-k">.</span>getStaticVariables()<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>Mout<span class="pl-pds">"</span></span>);
<span class="pl-smi">Variable</span> varW <span class="pl-k">=</span> bn<span class="pl-k">.</span>getStaticVariables()<span class="pl-k">.</span>getVariableByName(<span class="pl-s"><span class="pl-pds">"</span>W<span class="pl-pds">"</span></span>);

<span class="pl-c">//First we create an instance of a inference algorithm. </span>
<span class="pl-c">//In this case, we use the ImportanceSampling class.</span>
<span class="pl-smi">InferenceAlgorithm</span> inferenceAlgorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">HuginInference</span>();

<span class="pl-c">//Then, we set the BN model</span>
inferenceAlgorithm<span class="pl-k">.</span>setModel(bn);

<span class="pl-c">//If exists, we also set the evidence.</span>
<span class="pl-smi">Assignment</span> assignment <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-smi">HashMapAssignment</span>(<span class="pl-c1">1</span>);
assignment<span class="pl-k">.</span>setValue(varW,<span class="pl-c1">0</span>);
inferenceAlgorithm<span class="pl-k">.</span>setEvidence(assignment);

<span class="pl-c">//Then we run inference</span>
inferenceAlgorithm<span class="pl-k">.</span>runInference();

<span class="pl-c">//Then we query the posterior of</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>P(Mout|W=0) = <span class="pl-pds">"</span></span> <span class="pl-k">+</span> inferenceAlgorithm<span class="pl-k">.</span>getPosterior(varMout));

<span class="pl-c">//Or some more refined queries</span>
<span class="pl-smi">System</span><span class="pl-k">.</span>out<span class="pl-k">.</span>println(<span class="pl-s"><span class="pl-pds">"</span>P(0.7&lt;Mout&lt;3.5 | W=0) = <span class="pl-pds">"</span></span> 
   <span class="pl-k">+</span> inferenceAlgorithm<span class="pl-k">.</span>getExpectedValue(varMout, v <span class="pl-k">-</span><span class="pl-k">&gt;</span> (<span class="pl-c1">0.7</span> <span class="pl-k">&lt;</span> v <span class="pl-k">&amp;&amp;</span> v <span class="pl-k">&lt;</span> <span class="pl-c1">3.5</span>) <span class="pl-k">?</span> <span class="pl-c1">1.0</span> <span class="pl-k">:</span> <span class="pl-c1">0.0</span> ));</pre></div>

<p><a href="#documentation">[Back to Top]</a></p>

<h2>
<a id="moalink-" class="anchor" href="#moalink-" aria-hidden="true"><span class="octicon octicon-link"></span></a>MoaLink <a name="moalinkexample"></a>
</h2>

<h3>
<a id="amidst-classifiers-from-moa-" class="anchor" href="#amidst-classifiers-from-moa-" aria-hidden="true"><span class="octicon octicon-link"></span></a>AMIDST Classifiers from MOA <a name="moalinkclassifiersexample"></a>
</h3>

<p>The following command can be used to learn a Bayesian model with a latent Gaussian variable (HG) and a multinomial with 2 states (HM), as displayed in figure below. The VMP algorithm is used to learn the parameters of these two non-observed variables and make predictions over the class variable.</p>

<p align="center">
<img title="Taxonomy" src="http://amidst.github.io/toolbox/images/HODE.jpg" width="400">
</p>

<pre><code>java -Xmx512m -cp "../lib/*" -javaagent:../lib/sizeofag-1.0.0.jar 
moa.DoTask EvaluatePrequential -l \(bayes.AmidstClassifier -g 1 
-m 2\) -s generators.RandomRBFGenerator -i 10000 -f 1000 -q 1000
</code></pre>

<p><a href="#documentation">[Back to Top]</a></p>

<h3>
<a id="amidst-regression-from-moa-" class="anchor" href="#amidst-regression-from-moa-" aria-hidden="true"><span class="octicon octicon-link"></span></a>AMIDST Regression from MOA <a name="moalinkregressionsexample"></a>
</h3>

<p>It is possible to learn an enriched naive Bayes model for regression if the class label is of a continuous nature. The following command uses the model in Figure \ref{fig:HODE}(b) on a toy dataset from WEKA's collection of <a href="http://prdownloads.sourceforge.net/weka/datasets-numeric.jar">regression problems</a>.</p>

<p align="center">
<img title="Taxonomy" src="http://amidst.github.io/toolbox/images/regressionHODE.jpg" width="400">
</p>

<pre><code>java -Xmx512m -cp "../lib/*" -javaagent:../lib/sizeofag-1.0.0.jar 
moa.DoTask EvaluatePrequentialRegression -l bayes.AmidstRegressor
 -s (ArffFileStream -f ./quake.arff)
</code></pre>

<p>Note that the simpler the dataset the less complex the model should be. In this case, \texttt{quake.arff} is a very simple and small dataset that should probably be learn with a more simple classifier, that is, a high-bias-low-variance classifier, in order to avoid overfitting. This aims at providing a simple running example.</p>

<p><a href="#documentation">[Back to Top]</a></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/amidst/toolbox">AMIDST Toolbox 1.0</a> is maintained by <a href="https://github.com/amidst">amidst</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
